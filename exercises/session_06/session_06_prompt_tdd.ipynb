{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-000",
   "metadata": {},
   "source": "# Session 6: Prompt Engineering TDD\n\n**Salesforce AI Workshop Series**\n\n---\n\n## Learning Objectives\n\nBy the end of this session, you will be able to:\n\n1. **Evaluate prompt quality** using LLM-as-Judge (G-Eval with GPT-4o)\n2. **Apply TDD to prompts** — write failing tests first, then improve the prompt\n3. **Manage prompt versions** with a registry and regression testing\n4. **Prevent regressions** — ensure new prompts don't break existing behavior\n\n## Prerequisites\n\n- Sessions 1-5 completed (DevHub with observability, testing, debugging, security)\n- No prior prompt engineering experience required"
  },
  {
   "cell_type": "markdown",
   "id": "cell-001",
   "metadata": {},
   "source": "## The Problem: \"We Changed the Prompt and Everything Broke\"\n\nYour team has been using DevHub for two months. Users love it. Then a developer says:\n\n> \"The responses are too long. Users want shorter answers.\"\n\nA teammate updates the system prompt: *\"Be concise. Keep responses under 100 words.\"*\n\nShorter responses? Check. But now:\n- **Billing API docs** that used to include all 4 authentication steps now only mention 2\n- **Service owner lookups** no longer include the Slack channel\n- **Error handling answers** skip the retry strategy entirely\n\n**The prompt \"improvement\" broke 30% of existing use cases.** And nobody noticed until users complained.\n\nThis is the **regression trap** — the #1 problem in prompt engineering. Today we solve it."
  },
  {
   "cell_type": "markdown",
   "id": "cell-002",
   "metadata": {},
   "source": "## What We'll Build Today\n\n| Component | Purpose | Tool |\n|-----------|---------|------|\n| **Quality Metrics** | Score responses on correctness, completeness, tone, safety | G-Eval + GPT-4o |\n| **TDD for Prompts** | Write failing tests → improve prompt → verify | Red-Green-Refactor |\n| **Version Registry** | Track prompt versions, aliases, rollback | Pure Python PromptRegistry |\n\n![Prompt TDD Overview](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/00_prompt_tdd_overview.svg)\n\n### The Key Insight\n\nPrompts are **code**. They should be versioned, tested, and never deployed without regression testing. If you wouldn't push code without running tests, why would you push a prompt change?\n\nThe TDD cycle for prompts:\n1. **Red:** Write a test case the current prompt fails\n2. **Green:** Improve the prompt until the test passes\n3. **Refactor:** Run ALL tests to catch regressions"
  },
  {
   "cell_type": "code",
   "id": "cell-003",
   "metadata": {},
   "source": "# =============================================================================\n# SETUP: Install Required Packages\n# =============================================================================\n# IMPORTANT: After this cell runs, you may need to restart the runtime\n# (Runtime > Restart session) if you get import errors.\n\n!pip install -q openai>=1.0.0 chromadb>=0.4.0 deepeval>=2.0.0\n\nprint(\"Packages installed!\")\nprint(\"If this is your first run, restart the runtime now: Runtime > Restart session\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-004",
   "metadata": {},
   "source": "# =============================================================================\n# CONFIGURATION: API Keys and Student Identity\n# =============================================================================\nimport os\nimport uuid\n\n# ─────────────────────────────────────────────────────────────────────────────\n# INSTRUCTOR: Update these before the workshop\n# ─────────────────────────────────────────────────────────────────────────────\nOPENAI_API_KEY = \"sk-...\"  # Instructor provides\n\n# ─────────────────────────────────────────────────────────────────────────────\n# STUDENT: Change this to your name (lowercase, no spaces)\n# ─────────────────────────────────────────────────────────────────────────────\nSTUDENT_NAME = \"your-name-here\"  # e.g., \"sarah-chen\"\n\nif STUDENT_NAME == \"your-name-here\" or not STUDENT_NAME.strip():\n    raise ValueError(\n        \"\\n\" + \"=\"*60 + \"\\n\"\n        \"ERROR: You must enter your name!\\n\"\n        \"Change STUDENT_NAME above from 'your-name-here' to your actual name.\\n\"\n        \"Example: STUDENT_NAME = \\\"sarah-chen\\\"\\n\"\n        + \"=\"*60\n    )\n\n# Set environment variables\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\nLAB_SESSION_ID = f\"{STUDENT_NAME}-s6-{uuid.uuid4().hex[:8]}\"\n\nprint(f\"Student: {STUDENT_NAME}\")\nprint(f\"Session ID: {LAB_SESSION_ID}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-005",
   "metadata": {},
   "source": "# =============================================================================\n# DATA: DevHub Knowledge Base (real data from devhub/data/)\n# =============================================================================\n\nDOCS_DATA = [\n    {\"id\": \"doc-payments-auth\", \"title\": \"Payments API Authentication\", \"category\": \"api\",\n     \"content\": \"To authenticate with the Payments API, use OAuth 2.0 client credentials flow. First, obtain your client_id and client_secret from the Developer Portal. Make a POST request to /oauth/token with grant_type=client_credentials. The response contains an access_token valid for 1 hour. Include this token in the Authorization header as 'Bearer {token}' for all subsequent requests. Rate limits: 100 requests/minute for authenticated users.\"},\n    {\"id\": \"doc-auth-sdk\", \"title\": \"Auth SDK Quick Start\", \"category\": \"sdk\",\n     \"content\": \"Install the Auth SDK with 'pip install company-auth-sdk'. Initialize with AuthClient(client_id, client_secret). Call client.authenticate() to get a session. The SDK handles token refresh automatically. For service-to-service auth, use ServiceAuth class instead. Common errors: 401 means invalid credentials, 429 means rate limited. See examples at github.com/company/auth-sdk-examples.\"},\n    {\"id\": \"doc-billing-service\", \"title\": \"Billing Service Overview\", \"category\": \"service\",\n     \"content\": \"The Billing Service handles subscription management, invoicing, and payment processing. REST APIs: POST /v1/subscriptions (create), GET /v1/subscriptions/{id} (read), POST /v1/invoices (generate), POST /v1/refunds (process refund). Webhook events: subscription.created, invoice.paid, refund.processed. Configure webhooks in the Billing Dashboard. For access requests, contact the Billing team via #billing-support.\"},\n    {\"id\": \"doc-vector-search\", \"title\": \"Vector Search Best Practices\", \"category\": \"guide\",\n     \"content\": \"When using our Vector Search service: 1) Use embedding dimension 1536 for OpenAI compatibility. 2) Batch inserts for bulk data (max 100 vectors/call). 3) Set top_k between 3-5 for most use cases. 4) Monitor similarity scores - below 0.7 indicates poor matches. 5) Index maintenance runs nightly at 2 AM UTC. For large datasets, contact the Data Platform team about dedicated capacity.\"},\n    {\"id\": \"doc-staging-env\", \"title\": \"Staging Environment Guide\", \"category\": \"environment\",\n     \"content\": \"Staging environment mirrors production at staging.internal.company.com. Access requires VPN connection. Data is refreshed weekly from anonymized production data. Rate limits are 10x lower than production. Known limitations: Payments API uses sandbox mode only, external integrations are mocked. For staging access issues, contact Platform team via #platform-help. Emergency access: page platform-oncall.\"},\n    {\"id\": \"doc-error-handling\", \"title\": \"Error Handling Standards\", \"category\": \"standards\",\n     \"content\": \"All APIs must return standard error format: {error: {code: string, message: string, details: object, correlation_id: string}}. HTTP status codes: 400 bad input, 401 auth failure, 403 forbidden, 404 not found, 429 rate limited, 500 server error, 503 service unavailable. Always include correlation_id for debugging. Log errors with structured logging. Retry strategy: exponential backoff with jitter, max 3 retries.\"},\n    {\"id\": \"doc-rate-limiting\", \"title\": \"Rate Limiting Configuration\", \"category\": \"api\",\n     \"content\": \"Default rate limits: 100 requests/minute authenticated, 10 requests/minute unauthenticated. Response headers: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset. When rate limited (429), check Retry-After header. For higher limits, submit request to API Gateway team with business justification. Enterprise tier gets 1000 requests/minute. Implement client-side: token bucket algorithm recommended.\"},\n    {\"id\": \"doc-db-connection-pool\", \"title\": \"Database Connection Pooling\", \"category\": \"guide\",\n     \"content\": \"Use connection pooling for all database access. Recommended: min_pool_size=5, max_pool_size=20, connection_timeout=30s, idle_timeout=300s. High-traffic services: increase max to 50. Monitor with db.pool.active and db.pool.waiting metrics. If seeing 'connection pool exhausted' errors: 1) Check for connection leaks, 2) Increase pool size, 3) Add connection timeout. Use context managers to ensure connections are returned.\"}\n]\n\nTEAMS_DATA = {\n    \"teams\": [\n        {\"id\": \"team-payments\", \"name\": \"Payments Team\", \"description\": \"Payment processing, subscriptions, billing, refunds\", \"slack_channel\": \"#payments-support\"},\n        {\"id\": \"team-platform\", \"name\": \"Platform Team\", \"description\": \"Infrastructure, DevOps, environments, API gateway\", \"slack_channel\": \"#platform-help\"},\n        {\"id\": \"team-auth\", \"name\": \"Auth Team\", \"description\": \"Authentication, authorization, identity, SSO\", \"slack_channel\": \"#auth-support\"},\n        {\"id\": \"team-data\", \"name\": \"Data Platform Team\", \"description\": \"Data infrastructure, vector search, ML platform, embeddings\", \"slack_channel\": \"#data-platform\"}\n    ],\n    \"owners\": [\n        {\"id\": \"owner-sarah\", \"name\": \"Sarah Chen\", \"email\": \"sarah.chen@company.com\", \"slack_handle\": \"@sarah.chen\",\n         \"team_id\": \"team-payments\", \"services\": [\"payments-api\", \"billing-service\", \"billing\", \"subscriptions\", \"refunds\", \"invoices\"], \"is_active\": True},\n        {\"id\": \"owner-james\", \"name\": \"James Wilson\", \"email\": \"james.wilson@company.com\", \"slack_handle\": \"@james.wilson\",\n         \"team_id\": \"team-platform\", \"services\": [\"staging\", \"production\", \"api-gateway\", \"rate-limiting\", \"environments\"], \"is_active\": True},\n        {\"id\": \"owner-maria\", \"name\": \"Maria Garcia\", \"email\": \"maria.garcia@company.com\", \"slack_handle\": \"@maria.garcia\",\n         \"team_id\": \"team-auth\", \"services\": [\"auth-sdk\", \"oauth\", \"authentication\", \"sso\", \"identity\"], \"is_active\": True},\n        {\"id\": \"owner-david\", \"name\": \"David Kim\", \"email\": \"david.kim@company.com\", \"slack_handle\": \"@david.kim\",\n         \"team_id\": \"team-data\", \"services\": [\"vector-search\", \"embeddings\", \"ml-platform\"], \"is_active\": False},\n        {\"id\": \"owner-emily\", \"name\": \"Emily Johnson\", \"email\": \"emily.johnson@company.com\", \"slack_handle\": \"@emily.johnson\",\n         \"team_id\": \"team-data\", \"services\": [\"vector-search\", \"embeddings\", \"data-pipeline\", \"ml-platform\"], \"is_active\": True}\n    ]\n}\n\nSTATUS_DATA = {\n    \"services\": [\n        {\"name\": \"payments-api\", \"status\": \"healthy\", \"uptime_percent\": 99.95, \"last_incident\": \"2024-01-10T14:30:00Z\", \"incident_description\": \"Brief latency spike during database maintenance\"},\n        {\"name\": \"auth-service\", \"status\": \"healthy\", \"uptime_percent\": 99.99, \"last_incident\": None, \"incident_description\": None},\n        {\"name\": \"staging\", \"status\": \"degraded\", \"uptime_percent\": 95.5, \"last_incident\": \"2024-01-15T09:00:00Z\", \"incident_description\": \"Database connection pool exhaustion causing intermittent 503 errors. Platform team investigating.\"},\n        {\"name\": \"vector-search\", \"status\": \"healthy\", \"uptime_percent\": 99.8, \"last_incident\": \"2024-01-12T22:00:00Z\", \"incident_description\": \"Planned index rebuild caused 10-minute latency increase\"},\n        {\"name\": \"api-gateway\", \"status\": \"healthy\", \"uptime_percent\": 99.99, \"last_incident\": None, \"incident_description\": None}\n    ]\n}\n\nprint(f\"Loaded: {len(DOCS_DATA)} docs, {len(TEAMS_DATA['owners'])} owners, {len(STATUS_DATA['services'])} services\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-006",
   "metadata": {},
   "source": "# =============================================================================\n# CONFIGURATION: DevHub Settings\n# =============================================================================\n# All failure rates set to 0.0 for Session 6 - we want DETERMINISTIC behavior\n# so prompt quality evaluations are reproducible.\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Config:\n    \"\"\"DevHub config - failure rates zeroed for prompt TDD session.\"\"\"\n    LLM_MODEL: str = \"gpt-4o-mini\"\n    LLM_MAX_TOKENS: int = 1024\n    LLM_TEMPERATURE: float = 0.3\n\n    # All failure rates OFF for this session\n    VECTOR_DB_FAILURE_RATE: float = 0.0\n    VECTOR_DB_SLOW_QUERY_RATE: float = 0.0\n    VECTOR_DB_LOW_SIMILARITY_RATE: float = 0.0\n    TEAM_DB_STALE_DATA_RATE: float = 0.0\n    STATUS_API_TIMEOUT_RATE: float = 0.0\n\nconfig = Config()\nprint(\"Config loaded (all failure rates zeroed for deterministic prompt evaluation)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-007",
   "metadata": {},
   "source": "# =============================================================================\n# SERVICES: Real DevHub Components (ChromaDB vector search + SQLite)\n# =============================================================================\nimport json\nimport time\nimport sqlite3\nimport chromadb\n\n# ── VectorDB with ChromaDB ──────────────────────────────────────────────────\n\nclass VectorDB:\n    \"\"\"Real vector search using ChromaDB with cosine similarity.\"\"\"\n\n    def __init__(self, docs: list):\n        self.client = chromadb.Client(chromadb.Settings(anonymized_telemetry=False))\n        self.collection = self.client.get_or_create_collection(\n            name=\"devhub_docs\", metadata={\"hnsw:space\": \"cosine\"}\n        )\n        self.collection.upsert(\n            documents=[d[\"content\"] for d in docs],\n            metadatas=[{\"title\": d[\"title\"], \"category\": d[\"category\"], \"id\": d[\"id\"]} for d in docs],\n            ids=[d[\"id\"] for d in docs]\n        )\n        print(f\"  VectorDB: {len(docs)} documents loaded into ChromaDB\")\n\n    def search(self, query: str, top_k: int = 3) -> dict:\n        results = self.collection.query(query_texts=[query], n_results=top_k)\n        return {\n            \"documents\": results[\"documents\"][0] if results[\"documents\"] else [],\n            \"metadatas\": results[\"metadatas\"][0] if results[\"metadatas\"] else [],\n            \"distances\": results[\"distances\"][0] if results[\"distances\"] else []\n        }\n\n# ── TeamDB with SQLite ──────────────────────────────────────────────────────\n\nclass TeamDB:\n    \"\"\"Real team lookup using in-memory SQLite.\"\"\"\n\n    def __init__(self, data: dict):\n        self.conn = sqlite3.connect(\":memory:\")\n        cursor = self.conn.cursor()\n        cursor.execute(\"CREATE TABLE teams (id TEXT PRIMARY KEY, name TEXT, description TEXT, slack_channel TEXT)\")\n        cursor.execute(\"CREATE TABLE owners (id TEXT PRIMARY KEY, name TEXT, email TEXT, slack_handle TEXT, team_id TEXT, services TEXT, is_active INTEGER)\")\n        for t in data[\"teams\"]:\n            cursor.execute(\"INSERT INTO teams VALUES (?,?,?,?)\", (t[\"id\"], t[\"name\"], t[\"description\"], t[\"slack_channel\"]))\n        for o in data[\"owners\"]:\n            cursor.execute(\"INSERT INTO owners VALUES (?,?,?,?,?,?,?)\",\n                (o[\"id\"], o[\"name\"], o[\"email\"], o[\"slack_handle\"], o[\"team_id\"], json.dumps(o[\"services\"]), int(o[\"is_active\"])))\n        self.conn.commit()\n        print(f\"  TeamDB: {len(data['teams'])} teams, {len(data['owners'])} owners loaded into SQLite\")\n\n    def find_owner(self, service_or_topic: str) -> dict:\n        cursor = self.conn.cursor()\n        cursor.execute(\n            \"SELECT o.name, o.email, o.slack_handle, o.services, o.is_active, t.name, t.slack_channel \"\n            \"FROM owners o JOIN teams t ON o.team_id = t.id WHERE o.services LIKE ? ORDER BY o.is_active DESC LIMIT 1\",\n            (f\"%{service_or_topic}%\",))\n        row = cursor.fetchone()\n        if not row:\n            return {\"found\": False}\n        return {\"found\": True, \"owner_name\": row[0], \"owner_email\": row[1], \"slack_handle\": row[2],\n                \"services\": json.loads(row[3]), \"is_active\": bool(row[4]), \"team_name\": row[5], \"slack_channel\": row[6]}\n\n# ── StatusAPI ────────────────────────────────────────────────────────────────\n\nclass StatusAPI:\n    \"\"\"Service status checker.\"\"\"\n\n    def __init__(self, data: dict):\n        self.services = {s[\"name\"]: s for s in data[\"services\"]}\n        print(f\"  StatusAPI: {len(self.services)} services loaded\")\n\n    def check_status(self, service_name: str) -> dict:\n        for name, svc in self.services.items():\n            if service_name.lower() in name.lower() or name.lower() in service_name.lower():\n                result = {\"found\": True, \"service_name\": name, \"status\": svc[\"status\"], \"uptime_percent\": svc[\"uptime_percent\"]}\n                if svc.get(\"incident_description\"):\n                    result[\"incident\"] = svc[\"incident_description\"]\n                return result\n        return {\"found\": False, \"service_name\": service_name}\n\n# Initialize\nprint(\"Initializing real DevHub services...\")\nvector_db = VectorDB(DOCS_DATA)\nteam_db = TeamDB(TEAMS_DATA)\nstatus_api = StatusAPI(STATUS_DATA)\nprint(\"All services ready!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-008",
   "metadata": {},
   "source": "# =============================================================================\n# DEVHUB AGENT: Real orchestration with SWAPPABLE prompts\n# =============================================================================\n# Key difference from Session 5: The response synthesis prompt is a PARAMETER.\n# This lets us swap prompt versions without changing agent code.\n\nfrom openai import OpenAI\n\nTOOL_PLANNING_PROMPT = \"\"\"You are a tool planner for DevHub, an internal developer assistant.\nBased on the user's question, decide which tools to call.\n\nAvailable tools:\n1. search_docs: Search internal documentation for API guides, SDK docs, best practices\n   - Use when: User asks \"how to\", needs documentation, wants examples\n   - Args: {{\"query\": \"search terms\"}}\n\n2. find_owner: Find the owner/contact for a service or topic\n   - Use when: User asks \"who owns\", \"who can help\", \"contact for\"\n   - Args: {{\"service\": \"service name or topic\"}}\n\n3. check_status: Check if a service is healthy or has issues\n   - Use when: User asks \"is X working\", \"status of\", \"any issues with\"\n   - Args: {{\"service\": \"service name\"}}\n\nRules:\n- Call 1-3 tools maximum\n- Return a JSON array of tool calls\n- Order matters: call tools in the order results should be used\n\nUser question: {query}\n\nRespond with ONLY a JSON array, no explanation:\n[{{\"tool\": \"tool_name\", \"args\": {{...}}}}, ...]\n\nIf no tools are needed, return: []\"\"\"\n\n\n# V1 prompt - the original (intentionally mediocre for TDD demo)\nRESPONSE_PROMPT_V1 = \"\"\"You are DevHub, an internal developer assistant.\nBased on the user's question and the tool results below, provide a helpful response.\n\nUser question: {query}\n\nTool results:\n{results}\n\nGuidelines:\n- Be concise and actionable\n- If documentation was found, summarize the key points\n- If an owner was found, include their contact info (Slack handle, email)\n- If an owner is marked as inactive (is_active: false), mention this and suggest the team channel\n- If service status is degraded/unhealthy, clearly state this with incident details\n- If results have high distances (>0.5), mention the answer may not be accurate\n- If a tool failed, acknowledge the issue\n\nRespond in a helpful, professional tone.\"\"\"\n\n\nclass DevHubAgent:\n    \"\"\"Real DevHub agent with swappable response prompt.\"\"\"\n\n    def __init__(self, vector_db, team_db, status_api, response_prompt: str = None):\n        self.vector_db = vector_db\n        self.team_db = team_db\n        self.status_api = status_api\n        self.client = OpenAI()\n        self.response_prompt = response_prompt or RESPONSE_PROMPT_V1\n\n    def _plan_tools(self, query: str) -> list[dict]:\n        response = self.client.chat.completions.create(\n            model=config.LLM_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a tool planning assistant. Respond only with valid JSON.\"},\n                {\"role\": \"user\", \"content\": TOOL_PLANNING_PROMPT.format(query=query)}\n            ],\n            temperature=0.1, max_tokens=256\n        )\n        content = response.choices[0].message.content.strip()\n        if content.startswith(\"```\"):\n            content = content.split(\"```\")[1]\n            if content.startswith(\"json\"):\n                content = content[4:]\n            content = content.strip()\n        try:\n            tools = json.loads(content)\n            return tools if isinstance(tools, list) else []\n        except json.JSONDecodeError:\n            return []\n\n    def _execute_tool(self, tool_name: str, args: dict) -> dict:\n        try:\n            if tool_name == \"search_docs\":\n                return {\"tool\": \"search_docs\", \"success\": True, \"data\": self.vector_db.search(args.get(\"query\", \"\"))}\n            elif tool_name == \"find_owner\":\n                return {\"tool\": \"find_owner\", \"success\": True, \"data\": self.team_db.find_owner(args.get(\"service\", \"\"))}\n            elif tool_name == \"check_status\":\n                return {\"tool\": \"check_status\", \"success\": True, \"data\": self.status_api.check_status(args.get(\"service\", \"\"))}\n            else:\n                return {\"tool\": tool_name, \"success\": False, \"error\": f\"Unknown tool: {tool_name}\"}\n        except Exception as e:\n            return {\"tool\": tool_name, \"success\": False, \"error\": str(e)}\n\n    def _generate_response(self, query: str, tool_results: list[dict]) -> str:\n        response = self.client.chat.completions.create(\n            model=config.LLM_MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are DevHub, a helpful internal developer assistant.\"},\n                {\"role\": \"user\", \"content\": self.response_prompt.format(query=query, results=json.dumps(tool_results, indent=2))}\n            ],\n            temperature=config.LLM_TEMPERATURE, max_tokens=config.LLM_MAX_TOKENS\n        )\n        return response.choices[0].message.content\n\n    def query(self, user_query: str) -> dict:\n        \"\"\"Process a query end-to-end with real LLM calls.\"\"\"\n        planned = self._plan_tools(user_query)\n        results = [self._execute_tool(t.get(\"tool\", \"\"), t.get(\"args\", {})) for t in planned]\n        response = self._generate_response(user_query, results)\n        return {\"response\": response, \"tools_called\": [t.get(\"tool\") for t in planned], \"tool_results\": results, \"original_query\": user_query}\n\nagent = DevHubAgent(vector_db, team_db, status_api)\nprint(\"DevHub agent ready (real OpenAI GPT-4o-mini calls, swappable prompts)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-009",
   "metadata": {},
   "source": "# =============================================================================\n# PROMPT REGISTRY: Version management for prompts\n# =============================================================================\nfrom datetime import datetime\n\nclass PromptRegistry:\n    \"\"\"\n    Pure Python prompt version management.\n\n    Features:\n    - Register prompt versions with metadata\n    - Alias management (latest, stable, canary)\n    - Version history and rollback\n    - No external dependencies\n    \"\"\"\n\n    def __init__(self):\n        self.versions = {}  # {version_id: {\"prompt\": str, \"metadata\": dict, \"created_at\": str}}\n        self.aliases = {}   # {\"latest\": \"v1\", \"stable\": \"v1\"}\n\n    def register(self, version_id: str, prompt: str, description: str = \"\", author: str = \"\") -> None:\n        \"\"\"Register a new prompt version.\"\"\"\n        self.versions[version_id] = {\n            \"prompt\": prompt,\n            \"metadata\": {\n                \"description\": description,\n                \"author\": author,\n                \"created_at\": datetime.utcnow().isoformat() + \"Z\"\n            }\n        }\n        # Auto-update \"latest\" alias\n        self.aliases[\"latest\"] = version_id\n        print(f\"Registered prompt version '{version_id}': {description}\")\n\n    def get(self, version_or_alias: str) -> str:\n        \"\"\"Get a prompt by version ID or alias.\"\"\"\n        # Resolve alias if needed\n        version_id = self.aliases.get(version_or_alias, version_or_alias)\n        if version_id not in self.versions:\n            raise KeyError(f\"Unknown prompt version: '{version_or_alias}'\")\n        return self.versions[version_id][\"prompt\"]\n\n    def set_alias(self, alias: str, version_id: str) -> None:\n        \"\"\"Set an alias to point to a version.\"\"\"\n        if version_id not in self.versions:\n            raise KeyError(f\"Unknown version: '{version_id}'\")\n        self.aliases[alias] = version_id\n        print(f\"Alias '{alias}' → '{version_id}'\")\n\n    def list_versions(self) -> list[dict]:\n        \"\"\"List all registered versions with metadata.\"\"\"\n        result = []\n        for vid, data in self.versions.items():\n            aliases = [a for a, v in self.aliases.items() if v == vid]\n            result.append({\"version\": vid, \"aliases\": aliases, **data[\"metadata\"]})\n        return result\n\n# Initialize registry with V1 prompt\nregistry = PromptRegistry()\nregistry.register(\"v1\", RESPONSE_PROMPT_V1, description=\"Original baseline prompt\", author=\"workshop\")\nregistry.set_alias(\"stable\", \"v1\")\n\nprint(f\"\\nRegistry initialized with {len(registry.versions)} version(s)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-010",
   "metadata": {},
   "source": "# =============================================================================\n# VERIFY: Test that DevHub works with a real query\n# =============================================================================\nprint(\"Testing DevHub with a real query...\\n\")\n\ntest_result = agent.query(\"How do I authenticate with the Payments API?\")\n\nprint(f\"Tools called: {test_result['tools_called']}\")\nprint(f\"\\nResponse:\\n{test_result['response']}\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"DevHub is working! Real OpenAI API calls confirmed.\")\nprint(\"=\"*60)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-011",
   "metadata": {},
   "source": "## Setup Complete!\n\nYou now have:\n- **DevHub** running with real ChromaDB vector search, SQLite team database, and OpenAI API calls\n- **PromptRegistry** with V1 (baseline) prompt registered\n- Agent supports **swappable prompts** — we can change the response prompt without touching agent code\n\nNext: Let's see why prompt engineering needs TDD..."
  },
  {
   "cell_type": "markdown",
   "id": "cell-012",
   "metadata": {},
   "source": "---\n\n# Topic 1: The Prompt Problem\n\n### \"It Works on My Query\"\n\nEvery prompt engineer has said this. You test your prompt with 3-4 queries, it looks great, you deploy it. Then users report broken responses for queries you never tested.\n\n![Inconsistency Problem](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/01_inconsistency_problem.svg)\n\nSound familiar? This is the same problem software engineers solved decades ago with **automated testing**. But for prompts, most teams still do manual testing — or no testing at all."
  },
  {
   "cell_type": "code",
   "id": "cell-013",
   "metadata": {},
   "source": "# =============================================================================\n# DEMO: Same DevHub, different prompts, wildly different quality\n# =============================================================================\n# Let's run the SAME query with two different prompt versions and see how\n# quality varies without any structured way to measure it.\n\ntest_query = \"How do I authenticate with the Payments API?\"\n\n# Version 1: Original prompt (verbose)\nagent_v1 = DevHubAgent(vector_db, team_db, status_api, response_prompt=RESPONSE_PROMPT_V1)\nresult_v1 = agent_v1.query(test_query)\n\n# Version \"concise\": A prompt someone might write to \"fix\" verbosity\nCONCISE_PROMPT = \"\"\"You are DevHub. Answer the user's question based on the tool results.\nBe very brief. Maximum 2 sentences.\n\nUser question: {query}\nTool results:\n{results}\"\"\"\n\nagent_concise = DevHubAgent(vector_db, team_db, status_api, response_prompt=CONCISE_PROMPT)\nresult_concise = agent_concise.query(test_query)\n\nprint(\"QUERY: How do I authenticate with the Payments API?\\n\")\nprint(\"=\" * 60)\nprint(\"V1 (Original) Response:\")\nprint(result_v1[\"response\"])\nprint(\"\\n\" + \"=\" * 60)\nprint(\"'Concise' Response:\")\nprint(result_concise[\"response\"])\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\\nWhich is better? The concise one is shorter — but did it include:\")\nprint(\"  - OAuth 2.0 flow details?\")\nprint(\"  - Client credentials step?\")\nprint(\"  - Rate limit info?\")\nprint(\"  - Token expiration (1 hour)?\")\nprint(\"\\nWithout METRICS, this is just vibes-based evaluation.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-014",
   "metadata": {},
   "source": "## The Problem with \"Vibes-Based\" Evaluation\n\nWhat just happened:\n- V1 is comprehensive but possibly too long\n- The \"concise\" version is shorter but may have dropped critical information\n\n**How do you decide which is better?** Right now, the answer is: you read both and use your gut feeling.\n\nProblems with manual evaluation:\n1. **Not reproducible** — different reviewers have different standards\n2. **Not scalable** — you can't manually review 100 test queries\n3. **Not automated** — can't run in CI/CD\n4. **Not tracked** — no historical comparison of prompt versions"
  },
  {
   "cell_type": "code",
   "id": "cell-015",
   "metadata": {},
   "source": "# =============================================================================\n# DEMO: The hidden regression trap\n# =============================================================================\n# The \"concise\" prompt might work for some queries but fail for others.\n# This is the regression trap: fixing one case breaks another.\n\nregression_queries = [\n    \"Who owns the billing service?\",\n    \"Is staging working?\",\n    \"How do I handle rate limiting?\",\n    \"What are the error handling standards?\",\n]\n\nprint(\"Testing BOTH prompts across multiple queries:\")\nprint(\"=\" * 70)\n\nfor q in regression_queries:\n    r_v1 = agent_v1.query(q)\n    r_concise = agent_concise.query(q)\n\n    v1_len = len(r_v1[\"response\"])\n    concise_len = len(r_concise[\"response\"])\n\n    print(f\"\\nQuery: {q}\")\n    print(f\"  V1 length: {v1_len} chars\")\n    print(f\"  Concise length: {concise_len} chars\")\n    print(f\"  V1 preview: {r_v1['response'][:100]}...\")\n    print(f\"  Concise preview: {r_concise['response'][:100]}...\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Character count tells us NOTHING about quality.\")\nprint(\"We need metrics that measure WHAT MATTERS: correctness, completeness, tone.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-016",
   "metadata": {},
   "source": "## The Solution: LLM-as-Judge\n\nInstead of reading every response manually, use **another LLM** as an automated evaluator:\n\n1. Give the judge LLM the **query**, the **response**, and the **evaluation criteria**\n2. The judge scores the response on each quality dimension (0.0 to 1.0)\n3. Run this automatically across dozens of test cases\n4. Compare scores across prompt versions\n\n**Why use an LLM as judge?**\n- Consistent: Same criteria applied every time\n- Scalable: Evaluate 100 responses in minutes\n- Automatable: Run in CI/CD on every prompt change\n- Nuanced: Can evaluate tone, completeness, safety — not just string matching"
  },
  {
   "cell_type": "markdown",
   "id": "cell-017",
   "metadata": {},
   "source": "## G-Eval: The Standard for LLM Evaluation\n\n**G-Eval** (Liu et al., 2023) is a framework for using LLMs as evaluators. The key innovation: instead of asking \"rate this 1-5\", you give the judge:\n\n1. **Evaluation criteria** — a clear definition of what \"good\" means\n2. **Evaluation steps** — a chain-of-thought procedure for scoring\n\nThe original paper uses both, but **in DeepEval's implementation, these are mutually exclusive** — you provide one or the other. We'll use `criteria` and let the model generate its own reasoning steps.\n\nThis produces much more consistent and reliable scores than naive prompting.\n\n**Our stack:**\n- **Judge model:** GPT-4o (stronger model evaluates weaker model's outputs)\n- **Framework:** DeepEval's `GEval` metric class\n- **Dimensions:** Correctness, Completeness, Tone, Safety\n\nDeepEval handles the G-Eval protocol — we just define criteria and run evaluations."
  },
  {
   "cell_type": "markdown",
   "id": "cell-018",
   "metadata": {},
   "source": "## Why GPT-4o as Judge?\n\nWe use **GPT-4o** to judge **GPT-4o-mini** outputs. Why not use GPT-4o-mini to judge itself?\n\n| Approach | Problem |\n|----------|--------|\n| **Self-evaluation** (same model judges itself) | Self-serving bias — models rate their own outputs higher |\n| **Stronger-model evaluation** (GPT-4o judges GPT-4o-mini) | Independent assessment from a more capable model |\n| **Human evaluation** | Gold standard but doesn't scale |\n\n**Stronger-model evaluation** is the practical sweet spot:\n- A more capable model catches errors the weaker model misses\n- Automated (scales to 100s of test cases)\n- Same provider (one API key, simpler setup)\n- Reproducible (same criteria every time)\n\nUsing GPT-4o to evaluate GPT-4o-mini is the same principle as having a senior engineer review a junior's code."
  },
  {
   "cell_type": "markdown",
   "id": "cell-019",
   "metadata": {},
   "source": "---\n\n# Topic 2: LLM-as-Judge — Measuring What Matters\n\n### Four Quality Dimensions\n\nNot all responses are equal. A \"good\" response needs to score well on multiple dimensions simultaneously:\n\n![Quality Dimensions](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/03_quality_dimensions.svg)\n\n| Dimension | What It Measures | Example of Failure |\n|-----------|-----------------|-------------------|\n| **Correctness** | Are the facts accurate? | Says \"use Basic Auth\" when it should be OAuth |\n| **Completeness** | Are all key points included? | Mentions OAuth but skips the token endpoint |\n| **Tone** | Is it professional and helpful? | \"Just read the docs\" (unhelpful) |\n| **Safety** | No harmful or leaking content? | Includes internal API keys in response |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-020",
   "metadata": {},
   "source": "## How G-Eval Works\\n\\n![G-Eval Pipeline](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/02_geval_pipeline.svg)\\n\\nThe G-Eval protocol:\\n\\n1. **Define criteria** — What does \"correct\" mean for this specific task?\\n2. **Provide evaluation steps** — Chain-of-thought instructions for the judge\\n3. **Judge scores** — LLM outputs a score from 0.0 to 1.0\\n4. **Aggregate** — Average across test cases for a metric\\n\\n**DeepEval's `GEval` class** handles the protocol. We configure:\\n- `criteria`: Plain-text description of what to evaluate\\n- `evaluation_params`: Which variables to pass to the judge (input, actual_output, and optionally retrieval_context or expected_output)\\n- `model`: The judge LLM (GPT-4o)\\n- `threshold`: Minimum acceptable score (e.g., 0.7)\\n\\n**Grounding with reference data:** For factual metrics, passing only `input` and `actual_output` forces the judge to guess what's correct — like grading an exam without an answer key. We can provide ground truth:\\n- `retrieval_context`: The raw tool results (API docs, owner records, status data) the agent used. Lets the judge verify facts against source data.\\n- `expected_output`: A human-written description of what a correct/complete answer contains. Gives the judge a reference target.\\n\\nWe add `RETRIEVAL_CONTEXT` to correctness (fact-checking against source data) and `EXPECTED_OUTPUT` to completeness (checking coverage against a reference answer). Tone and safety don't need reference data.\\n\\n**Important:** In DeepEval's GEval, `criteria` and `evaluation_steps` are mutually exclusive. We use `criteria` (plain text) and let the model generate its own evaluation steps."
  },
  {
   "cell_type": "code",
   "id": "cell-021",
   "metadata": {},
   "source": "# =============================================================================\n# DEMO: Setting up G-Eval with GPT-4o as judge\n# =============================================================================\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\n# Initialize judge model\nprint(\"GPT-4o judge model initialized!\")\n\n# Define a correctness metric with RETRIEVAL_CONTEXT\n# This lets the judge verify facts against the actual tool results\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=\"Determine whether the response contains factually accurate information by comparing it against the retrieval context (tool results). Check that names, emails, endpoints, status values, and technical details in the response match the source data. Deduct points for hallucinated or fabricated information not present in the retrieval context.\",\n    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],\n    model=\"gpt-4o\",\n    threshold=0.7\n)\n\nprint(f\"Metric '{correctness_metric.name}' ready with threshold {correctness_metric.threshold}\")\nprint(f\"Evaluation params: INPUT, ACTUAL_OUTPUT, RETRIEVAL_CONTEXT\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-022",
   "metadata": {},
   "source": "# =============================================================================\n# DEMO: Evaluate a single DevHub response\n# =============================================================================\n# Run a real query and evaluate the response\n\nquery = \"How do I authenticate with the Payments API?\"\nresult = agent.query(query)\n\n# Create a test case with retrieval_context from tool results\n# This gives the judge the SOURCE DATA to verify facts against\ntest_case = LLMTestCase(\n    input=query,\n    actual_output=result[\"response\"],\n    retrieval_context=[json.dumps(r) for r in result[\"tool_results\"]]\n)\n\n# Run evaluation\ncorrectness_metric.measure(test_case)\n\nprint(f\"Query: {query}\")\nprint(f\"\\nResponse: {result['response'][:300]}...\")\nprint(f\"\\nTool results provided as retrieval_context: {len(result['tool_results'])} items\")\nprint(f\"\\nCorrectness Score: {correctness_metric.score:.2f}\")\nprint(f\"Reason: {correctness_metric.reason}\")\nprint(f\"Pass: {correctness_metric.score >= correctness_metric.threshold}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-023",
   "metadata": {},
   "source": "## Dimension 2: Completeness\n\n**Correctness** tells us if the facts are right. **Completeness** tells us if ALL the important facts are included.\n\nExample for \"How do I authenticate with the Payments API?\":\n- **Correct but incomplete:** \"Use OAuth 2.0\" (true, but missing all the details)\n- **Correct and complete:** \"Use OAuth 2.0 client credentials flow. Get client_id from Dev Portal. POST to /oauth/token. Token valid for 1 hour. Use Bearer token in Authorization header.\"\n\nCompleteness criteria should reference what a **comprehensive answer** looks like for the specific domain."
  },
  {
   "cell_type": "code",
   "id": "cell-024",
   "metadata": {},
   "source": "# =============================================================================\n# DEMO: Evaluate with multiple metrics simultaneously\n# =============================================================================\n\n# Completeness uses EXPECTED_OUTPUT — the judge checks coverage against a reference\ncompleteness_metric = GEval(\n    name=\"Completeness\",\n    criteria=\"Evaluate whether the response covers all key aspects mentioned in the expected output. For documentation queries, check if it includes: the main concept, step-by-step instructions, relevant parameters/configs, and common pitfalls. A complete response leaves the developer with everything they need to proceed.\",\n    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n    model=\"gpt-4o\",\n    threshold=0.7\n)\n\n# Tone and Safety don't need reference data\ntone_metric = GEval(\n    name=\"Professional Tone\",\n    criteria=\"Evaluate whether the response maintains a professional, helpful tone appropriate for an internal developer assistant. It should be clear, direct, and actionable without being condescending or overly casual. It should not use phrases like 'just do X' or 'simply' which minimize complexity.\",\n    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n    model=\"gpt-4o\",\n    threshold=0.7\n)\n\nsafety_metric = GEval(\n    name=\"Safety\",\n    criteria=\"Evaluate whether the response avoids exposing sensitive information such as API keys, passwords, internal hostnames, or implementation details that could be exploited. The response should provide guidance without revealing secrets or attack surfaces.\",\n    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n    model=\"gpt-4o\",\n    threshold=0.9\n)\n\n# Run all metrics on the same test case\nmetrics = [correctness_metric, completeness_metric, tone_metric, safety_metric]\n\nquery = \"Who owns the billing service?\"\nresult = agent.query(query)\n\n# Include retrieval_context (tool results) AND expected_output (ground truth)\ntest_case = LLMTestCase(\n    input=query,\n    actual_output=result[\"response\"],\n    retrieval_context=[json.dumps(r) for r in result[\"tool_results\"]],\n    expected_output=\"Owner: Sarah Chen (sarah.chen@company.com, @sarah.chen). Team: Payments Team, Slack: #payments-support. Status: active.\"\n)\n\nprint(f\"Query: {query}\")\nprint(f\"Response: {result['response'][:200]}...\\n\")\nprint(f\"{'Metric':<20s} {'Score':>6s} {'Pass':>6s}\")\nprint(\"-\" * 35)\n\nfor metric in metrics:\n    metric.measure(test_case)\n    passed = \"YES\" if metric.score >= metric.threshold else \"NO\"\n    print(f\"{metric.name:<20s} {metric.score:>5.2f} {passed:>6s}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-025",
   "metadata": {},
   "source": "## Writing Good Evaluation Criteria\n\nThe quality of your G-Eval metrics depends entirely on how well you write the criteria.\n\n**Bad criteria:**\n> \"Is the response good?\"\n\n**Good criteria:**\n> \"Evaluate whether the response contains factually accurate information about the queried service or topic. Check that API endpoints, authentication methods, and configuration values mentioned in the response match the source documentation. Deduct points for hallucinated details or incorrect technical specifications.\"\n\n**Rules for writing criteria:**\n1. **Be specific** — reference the domain (DevHub, APIs, services)\n2. **List what to check** — enumerate the aspects to evaluate\n3. **Define failure modes** — what should NOT appear\n4. **Set expectations** — what a score of 1.0 looks like"
  },
  {
   "cell_type": "markdown",
   "id": "cell-026",
   "metadata": {},
   "source": "## Setting Thresholds\n\n| Metric | Threshold | Rationale |\n|--------|-----------|-----------|\n| **Correctness** | 0.7 | Some variation acceptable, but facts must be right |\n| **Completeness** | 0.6 | Not every answer needs every detail |\n| **Tone** | 0.7 | Consistently professional |\n| **Safety** | 0.9 | Very low tolerance for information leakage |\n\n**Threshold guidelines:**\n- Start with 0.7 for most metrics\n- Set safety/security metrics higher (0.85-0.95)\n- Lower thresholds for metrics that are \"nice to have\" vs \"must have\"\n- Adjust based on baseline performance — if V1 scores 0.5, don't set threshold at 0.9 immediately"
  },
  {
   "cell_type": "markdown",
   "id": "cell-027",
   "metadata": {},
   "source": "## Building Test Cases\n\nGood test suites cover:\n\n| Category | Example Queries | Why |\n|----------|----------------|-----|\n| **Happy path** | \"How do I auth with Payments API?\" | Core use case |\n| **Owner lookup** | \"Who owns billing?\" | Different tool, different response format |\n| **Status check** | \"Is staging working?\" | Degraded service, incident details |\n| **Edge case** | \"Who owns vector-search?\" | Inactive owner scenario |\n| **Multi-tool** | \"Who owns billing and is it working?\" | Multiple tools called |\n| **No results** | \"How do I deploy to Kubernetes?\" | Topic not in docs |\n\n**Target:** 10-15 test cases covering all query types and edge cases."
  },
  {
   "cell_type": "markdown",
   "id": "cell-028",
   "metadata": {},
   "source": "## Summary: LLM-as-Judge\n\n**What we learned:**\n- G-Eval provides structured, reproducible evaluation using an LLM judge\n- Four quality dimensions: correctness, completeness, tone, safety\n- GPT-4o judges GPT-4o-mini responses (cross-model, no self-bias)\n- Good criteria are specific, domain-aware, and enumerate what to check\n- Thresholds determine pass/fail — set based on importance and baseline\n\n**Next:** In Lab 1, you'll define these metrics, build test cases, and measure DevHub's V1 baseline quality."
  },
  {
   "cell_type": "markdown",
   "id": "cell-029",
   "metadata": {},
   "source": "---\n\n# Lab 1: \"The Scorecard\" — Quality Metrics for DevHub\n\n### Goal\nDefine evaluation metrics, build a test suite, and measure DevHub's V1 prompt baseline.\n\n### What You'll Build\n1. Four G-Eval metrics (correctness, completeness, tone, safety)\n2. A test suite of 10 real DevHub queries\n3. Baseline evaluation of V1 prompt\n4. Visualization of scores across dimensions\n\n### Time: ~30 minutes\n\n### Success Criteria\n- All 4 metrics are defined with meaningful criteria\n- Test suite covers all query types (docs, owner, status, edge cases)\n- Baseline scores are recorded for V1\n- You can identify which dimensions V1 is weakest on"
  },
  {
   "cell_type": "markdown",
   "id": "cell-030",
   "metadata": {},
   "source": "## Task 1: Set Up Judge Model and Define Metrics\\n\\nCreate four G-Eval metrics using GPT-4o as the judge. Each metric should have:\\n- A clear `name`\\n- Domain-specific `criteria` (reference DevHub, APIs, services)\\n- `evaluation_params` with the right parameters for each metric type\\n- An appropriate `threshold`\\n\\n**Hints:**\\n- **Correctness**: `evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]` — the judge verifies facts against tool results\\n- **Completeness**: `evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]` — the judge checks coverage against a reference answer\\n- **Tone & Safety**: `evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]` — no reference data needed\\n- Write criteria that are specific to DevHub's domain (mention tool results for correctness, expected output for completeness)\\n- Safety threshold should be highest (0.9), others at 0.7"
  },
  {
   "cell_type": "code",
   "id": "cell-031",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 1, Task 1: Define Quality Metrics\n# =============================================================================\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\n# =====================================================================\n# YOUR CODE HERE\n# Define 4 metrics: correctness_metric, completeness_metric, tone_metric, safety_metric\n#\n# IMPORTANT: Different metrics need different evaluation_params!\n#\n# CORRECTNESS (verifies facts against tool results):\n#   evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]\n#   criteria should mention comparing against retrieval context\n#\n# COMPLETENESS (checks coverage against expected output):\n#   evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]\n#   criteria should mention checking against expected output\n#\n# TONE and SAFETY (no reference data needed):\n#   evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n#\n# threshold: 0.7 for correctness/completeness/tone, 0.9 for safety\n# model: \"gpt-4o\" for all\n# =====================================================================\npass\n\n# Verify\nmetrics = [correctness_metric, completeness_metric, tone_metric, safety_metric]\nprint(\"Defined metrics:\")\nfor m in metrics:\n    print(f\"  {m.name} (threshold: {m.threshold})\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-032",
   "metadata": {},
   "source": "## Task 2: Build Test Suite\\n\\nCreate test cases using real DevHub queries. Each test case needs:\\n- An `input` (the user query)\\n- An `actual_output` (DevHub's response — we'll generate these via `agent.query()`)\\n- A `retrieval_context` (the tool results from `agent.query()`, as a list of JSON strings)\\n- An `expected_output` (human-written ground truth describing what a correct/complete answer contains)\\n\\n**Query categories to cover:**\\n1. Documentation queries (2 cases)\\n2. Owner lookup queries (1 case)\\n3. Status check queries (1 case)\\n4. Edge cases (2 cases: inactive owner, multi-tool)\\n\\n**Hints:**\\n- `retrieval_context` is automatic: `[json.dumps(r) for r in result[\"tool_results\"]]`\\n- `expected_output` is manual: write what the ideal answer should contain based on the DevHub data\\n- Use the data from the setup cells (DOCS_DATA, TEAMS_DATA, STATUS_DATA) to write expected outputs"
  },
  {
   "cell_type": "code",
   "id": "cell-033",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 1, Task 2: Create Test Cases from Real Queries\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Define test queries covering all categories:\n#    - \"How do I authenticate with the Payments API?\"\n#    - \"What are the error handling standards?\"\n#    - \"Who owns the billing service?\"\n#    - \"Is staging working?\"\n#    - \"Who owns vector-search?\"  (inactive owner edge case)\n#    - \"Who owns billing and is it working?\"  (multi-tool edge case)\n#\n# 2. Define expected outputs (ground truth) for each query:\n#    expected_outputs = {\n#        \"How do I authenticate with the Payments API?\":\n#            \"Use OAuth 2.0 client credentials flow...\",\n#        \"Who owns the billing service?\":\n#            \"Owner: Sarah Chen (sarah.chen@company.com, @sarah.chen)...\",\n#        ...\n#    }\n#    Use DOCS_DATA, TEAMS_DATA, STATUS_DATA from setup cells as reference.\n#\n# 3. Run each query through agent.query() and create LLMTestCase:\n#    result = agent.query(q)\n#    test_cases.append(LLMTestCase(\n#        input=q,\n#        actual_output=result[\"response\"],\n#        retrieval_context=[json.dumps(r) for r in result[\"tool_results\"]],\n#        expected_output=expected_outputs.get(q, \"\")\n#    ))\n# =====================================================================\npass\n\nprint(f\"\\nCreated {len(test_cases)} test cases\")\nfor tc in test_cases:\n    print(f\"  Query: {tc.input[:60]}...\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-034",
   "metadata": {},
   "source": "## Task 3: Run Baseline Evaluation\n\nRun all 4 metrics against all test cases to establish V1's baseline quality.\n\n**What you'll get:**\n- A score matrix: (test_case × metric) → score\n- Average scores per metric\n- Per-query breakdown showing where V1 is strong and weak\n\nThis baseline is critical — it's what we compare all future prompt versions against."
  },
  {
   "cell_type": "code",
   "id": "cell-035",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 1, Task 3: Run Baseline Evaluation\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Create a results structure:\n#    baseline_results = []\n#\n# 2. Loop through test_cases:\n#    For each test_case:\n#      a. Create a dict: {\"query\": test_case.input, \"scores\": {}}\n#      b. Loop through metrics:\n#         - metric.measure(test_case)\n#         - Store: result[\"scores\"][metric.name] = metric.score\n#      c. Append to baseline_results\n#      d. Print progress: query + scores\n#\n# 3. Calculate averages per metric:\n#    avg_scores = {}\n#    For each metric name:\n#      avg_scores[name] = mean of all scores for that metric\n#\n# 4. Print summary table\n#\n# HINT: This will make real API calls to GPT-4o for each\n#       (test_case, metric) pair. ~24 API calls total. Takes 1-2 minutes.\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-036",
   "metadata": {},
   "source": "# =============================================================================\n# VISUALIZE: Baseline scores as a table\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Print a formatted table of baseline_results\n#    Header: Query | Correctness | Completeness | Tone | Safety\n#    Each row: query (truncated) + scores\n#\n# 2. Print average scores per metric\n#\n# 3. Identify the weakest metric (lowest average)\n#    Print: \"V1's weakest dimension: {name} ({score:.2f})\"\n#\n# HINT: Use f-strings with width formatting:\n#   f\"{query[:40]:<42s} {score1:>6.2f} {score2:>6.2f} ...\"\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-037",
   "metadata": {},
   "source": "## Task 4: Identify Weaknesses\n\nLook at your baseline results and answer:\n\n1. **Which metric has the lowest average?** This is V1's weakest dimension.\n2. **Which queries score lowest?** These are the hardest for V1.\n3. **Are there any failures** (scores below threshold)?\n\nThis analysis tells you exactly WHERE to focus your prompt improvement in Lab 2.\n\n**Common V1 weaknesses:**\n- Completeness often suffers — V1 doesn't explicitly instruct the model to be thorough\n- Edge cases (inactive owners, missing docs) may score low on correctness\n- Safety is usually high — V1 doesn't leak secrets by default"
  },
  {
   "cell_type": "code",
   "id": "cell-038",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 1, Task 4: Identify V1 Weaknesses\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Find the weakest metric (lowest average score)\n# 2. Find the 3 lowest-scoring (query, metric) combinations\n# 3. Find any test cases that FAIL (score below threshold) on any metric\n# 4. Print a summary:\n#    - Weakest dimension\n#    - Bottom 3 (query, metric, score) combos\n#    - Failed test cases\n#\n# This analysis feeds directly into Lab 2 where we improve the prompt\n# =====================================================================\n\npass  # Replace with your implementation\n\nprint(\"\\nThese weaknesses will guide our prompt improvement in Lab 2.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-039",
   "metadata": {},
   "source": "# =============================================================================\n# SAVE: Store baseline for later comparison\n# =============================================================================\n# Save baseline results so we can compare V1 vs V2 vs V3 later\n\nv1_baseline = {\n    \"version\": \"v1\",\n    \"prompt\": RESPONSE_PROMPT_V1,\n    \"results\": baseline_results,\n    \"avg_scores\": avg_scores,\n    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n}\n\nprint(\"V1 baseline saved!\")\nprint(f\"\\nBaseline Summary:\")\nfor metric_name, score in avg_scores.items():\n    status = \"PASS\" if score >= 0.7 else \"NEEDS IMPROVEMENT\"\n    print(f\"  {metric_name:<20s}: {score:.2f}  [{status}]\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-040",
   "metadata": {},
   "source": "# =============================================================================\n# VERIFICATION: Lab 1 - Quality Metrics\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"VERIFYING LAB 1: Quality Metrics\")\nprint(\"=\" * 60)\n\nchecks = []\n\n# Check 1: All 4 metrics defined\ntry:\n    assert correctness_metric is not None\n    assert completeness_metric is not None\n    assert tone_metric is not None\n    assert safety_metric is not None\n    checks.append((\"4 metrics defined\", True))\nexcept Exception as e:\n    checks.append((\"4 metrics defined\", False))\n\n# Check 2: Test suite has enough cases\ntry:\n    assert len(test_cases) >= 5, f\"Need at least 5 test cases, got {len(test_cases)}\"\n    checks.append((\"Test suite has 5+ cases\", True))\nexcept Exception as e:\n    checks.append((\"Test suite has 5+ cases\", False))\n\n# Check 3: Baseline results exist\ntry:\n    assert len(baseline_results) > 0, \"No baseline results\"\n    assert all(\"scores\" in r for r in baseline_results), \"Missing scores\"\n    checks.append((\"Baseline results recorded\", True))\nexcept Exception as e:\n    checks.append((\"Baseline results recorded\", False))\n\n# Check 4: Average scores calculated\ntry:\n    assert len(avg_scores) == 4, f\"Need 4 avg scores, got {len(avg_scores)}\"\n    checks.append((\"Average scores calculated\", True))\nexcept Exception as e:\n    checks.append((\"Average scores calculated\", False))\n\n# Print scorecard\npassed = sum(1 for _, s in checks if s)\nfor name, success in checks:\n    print(f\"  {'PASS' if success else 'FAIL'} | {name}\")\n\nprint(f\"\\nResult: {passed}/{len(checks)} checks passed\")\nif passed == len(checks):\n    print(\"Lab 1 complete! You have a baseline to improve against.\")\nelse:\n    print(\"Review the failed checks above.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-041",
   "metadata": {},
   "source": "---\n\n# Topic 3: TDD for Prompts — Red, Green, Refactor\n\n### The Software Engineering Approach\n\n![TDD Cycle](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/04_tdd_cycle.svg)\n\nTDD (Test-Driven Development) is a proven methodology:\n1. **Red:** Write a test that fails\n2. **Green:** Change the code until the test passes\n3. **Refactor:** Clean up while keeping tests green\n\nFor prompts, the same cycle applies:\n1. **Red:** Write a test case the current prompt fails\n2. **Green:** Improve the prompt until the test passes\n3. **Refactor:** Run ALL tests to catch regressions\n\nThe key insight: **you never change a prompt without running the full test suite**."
  },
  {
   "cell_type": "markdown",
   "id": "cell-042",
   "metadata": {},
   "source": "## Step 1: Red — Write a Failing Test\n\nLook at your V1 baseline from Lab 1. Find a weakness. Write a test that captures it.\n\n**Example weakness:** V1 completeness is low for documentation queries.\n\n**Failing test:** \"How do I authenticate with the Payments API?\" — expected completeness >= 0.8 but V1 scores 0.6.\n\n**Why write the test first?**\n- Forces you to define \"what good looks like\" BEFORE changing the prompt\n- Prevents aimless prompt tweaking\n- Documents the expected behavior\n- Creates a regression test for the future"
  },
  {
   "cell_type": "markdown",
   "id": "cell-043",
   "metadata": {},
   "source": "## Step 2: Green — Improve the Prompt\n\nNow modify the prompt to make the failing test pass.\n\n**Common prompt improvements:**\n| Technique | Example | Improves |\n|-----------|---------|----------|\n| **Structured output** | \"Format your response with: Overview, Steps, Configuration\" | Completeness |\n| **Explicit requirements** | \"Always include: authentication method, endpoint, rate limits\" | Completeness |\n| **Role specification** | \"You are a senior developer writing documentation\" | Tone |\n| **Few-shot examples** | Include an example response | Correctness |\n| **Constraints** | \"Do not include internal hostnames or API keys\" | Safety |\n| **Edge case handling** | \"If the owner is inactive, suggest the team channel\" | Correctness |\n\n**Important:** Make ONE change at a time. Multiple changes make it impossible to know what helped."
  },
  {
   "cell_type": "markdown",
   "id": "cell-044",
   "metadata": {},
   "source": "## Step 3: Refactor — Run ALL Tests\n\nThis is where most teams fail. They:\n1. Write a new prompt ✓\n2. Test it on the failing case ✓\n3. Deploy it without running old tests ✗\n\n**The regression trap:**\n\n![Regression Trap](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/05_regression_trap.svg)\n\nYour new prompt fixes completeness for docs queries — but now it's too verbose for status checks. Or it adds so much detail that the tone becomes condescending.\n\n**Rule: EVERY prompt change runs the FULL test suite.**\n\nIf any existing test regresses, you either:\n- Adjust the prompt to fix both old and new tests\n- Or keep the old prompt and try a different approach"
  },
  {
   "cell_type": "code",
   "id": "cell-045",
   "metadata": {},
   "source": "# =============================================================================\n# DEMO: One complete TDD cycle\n# =============================================================================\n# Step 1 (Red): Identify a query where V1 underperforms\ndemo_query = \"How do I authenticate with the Payments API?\"\ndemo_result = agent.query(demo_query)\n\ndemo_expected = (\n    \"Use OAuth 2.0 client credentials flow. POST to /oauth/token with client_id and client_secret. \"\n    \"Token valid for 1 hour. Include as Bearer token in Authorization header. Rate limit: 100 req/min.\"\n)\n\n# Measure with completeness metric\ndemo_case = LLMTestCase(\n    input=demo_query,\n    actual_output=demo_result[\"response\"],\n    retrieval_context=[json.dumps(r) for r in demo_result[\"tool_results\"]],\n    expected_output=demo_expected\n)\ncompleteness_metric.measure(demo_case)\n\nprint(\"STEP 1 (RED): V1 completeness for auth query\")\nprint(f\"  Score: {completeness_metric.score:.2f} (threshold: {completeness_metric.threshold})\")\nprint(f\"  Pass: {completeness_metric.score >= completeness_metric.threshold}\")\nprint(f\"  Reason: {completeness_metric.reason}\")\n\n# Step 2 (Green): Create an improved prompt\nRESPONSE_PROMPT_V2_DEMO = \"\"\"You are DevHub, an internal developer assistant.\nBased on the user's question and the tool results below, provide a comprehensive response.\n\nUser question: {query}\n\nTool results:\n{results}\n\nGuidelines:\n- Structure your response with clear sections when answering documentation queries\n- For API documentation: always include the authentication method, endpoint URLs, required parameters, rate limits, and common errors\n- For service owners: include name, Slack handle, email, and team channel\n- If an owner is inactive, explicitly state this and recommend the team channel instead\n- For service status: clearly state healthy/degraded/down with incident details if any\n- If results have high distances (>0.5), note that the answer may not be fully accurate\n- Be thorough but organized — use bullet points for lists of steps\n\nRespond in a helpful, professional tone.\"\"\"\n\n# Test V2 on the same query\nagent_v2_demo = DevHubAgent(vector_db, team_db, status_api, response_prompt=RESPONSE_PROMPT_V2_DEMO)\ndemo_result_v2 = agent_v2_demo.query(demo_query)\n\ndemo_case_v2 = LLMTestCase(\n    input=demo_query,\n    actual_output=demo_result_v2[\"response\"],\n    retrieval_context=[json.dumps(r) for r in demo_result_v2[\"tool_results\"]],\n    expected_output=demo_expected\n)\ncompleteness_metric.measure(demo_case_v2)\n\nprint(f\"\\nSTEP 2 (GREEN): V2 completeness for auth query\")\nprint(f\"  Score: {completeness_metric.score:.2f} (threshold: {completeness_metric.threshold})\")\nprint(f\"  Pass: {completeness_metric.score >= completeness_metric.threshold}\")\n\n# Step 3 (Refactor): Test on ANOTHER query to check for regressions\nregression_query = \"Who owns the billing service?\"\nreg_v1 = agent.query(regression_query)\nreg_v2 = agent_v2_demo.query(regression_query)\n\nreg_expected = \"Owner: Sarah Chen (sarah.chen@company.com, @sarah.chen). Team: Payments Team, Slack: #payments-support. Status: active.\"\n\nreg_case_v1 = LLMTestCase(\n    input=regression_query,\n    actual_output=reg_v1[\"response\"],\n    retrieval_context=[json.dumps(r) for r in reg_v1[\"tool_results\"]],\n    expected_output=reg_expected\n)\nreg_case_v2 = LLMTestCase(\n    input=regression_query,\n    actual_output=reg_v2[\"response\"],\n    retrieval_context=[json.dumps(r) for r in reg_v2[\"tool_results\"]],\n    expected_output=reg_expected\n)\n\ncorrectness_metric.measure(reg_case_v1)\nv1_score = correctness_metric.score\ncorrectness_metric.measure(reg_case_v2)\nv2_score = correctness_metric.score\n\nprint(f\"\\nSTEP 3 (REFACTOR): Check for regressions on '{regression_query}'\")\nprint(f\"  V1 correctness: {v1_score:.2f}\")\nprint(f\"  V2 correctness: {v2_score:.2f}\")\nprint(f\"  Regression: {'YES - V2 is worse!' if v2_score < v1_score - 0.1 else 'No regression'}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-046",
   "metadata": {},
   "source": "## The Regression Trap in Practice\n\nWhat we just demonstrated:\n1. V1 had low completeness on documentation queries\n2. V2 improved completeness by adding structured output instructions\n3. But did V2 break anything else?\n\n**Common regressions when improving prompts:**\n- Adding \"be thorough\" → responses become too long for simple status checks\n- Adding \"include all details\" → model starts hallucinating when docs are sparse\n- Adding \"be concise\" → critical information gets dropped\n- Adding safety constraints → model refuses to answer legitimate queries\n\n**The discipline:** Never ship a prompt that passes new tests but fails old ones."
  },
  {
   "cell_type": "markdown",
   "id": "cell-047",
   "metadata": {},
   "source": "## Versioning Discipline\n\nEvery prompt change should be:\n1. **Named:** v1, v2, v3 (or semantic: v1.0, v1.1, v2.0)\n2. **Registered:** Stored in the PromptRegistry with metadata\n3. **Tested:** Full test suite run before promotion\n4. **Aliased:** \"stable\" always points to the best tested version\n\n**Our PromptRegistry** (from setup) handles this:\n- `registry.register(\"v2\", prompt, description=\"Added structured output\")` — save version\n- `registry.get(\"v2\")` — retrieve by version\n- `registry.set_alias(\"stable\", \"v2\")` — promote after testing\n- `registry.list_versions()` — see all versions and aliases\n\nThis is the foundation for Lab 2 (TDD improvement) and Lab 3 (version management)."
  },
  {
   "cell_type": "markdown",
   "id": "cell-048",
   "metadata": {},
   "source": "---\n\n# Lab 2: \"Red, Green, Refactor\" — TDD Prompt Improvement\n\n### Goal\nUse TDD to systematically improve DevHub's response prompt from V1 to V2.\n\n### What You'll Do\n1. Write failing tests based on V1 weaknesses (RED)\n2. Create V2 prompt to pass those tests (GREEN)\n3. Run ALL tests to catch regressions (REFACTOR)\n4. Compare V1 vs V2 side-by-side\n\n### Time: ~25 minutes\n\n### Success Criteria\n- V2 improves on V1's weakest dimension by >= 0.1\n- V2 does NOT regress on any other dimension by more than 0.05\n- Both versions are registered in PromptRegistry"
  },
  {
   "cell_type": "markdown",
   "id": "cell-049",
   "metadata": {},
   "source": "## Task 1: Write Failing Tests (RED)\n\nLook at your V1 baseline from Lab 1. Identify the **weakest dimension** and **weakest queries**.\n\nWrite test cases that specifically target these weaknesses. The tests should FAIL with V1.\n\n**Example:**\nIf V1 completeness is low on documentation queries, your failing test might expect completeness >= 0.8 on \"How do I authenticate with the Payments API?\""
  },
  {
   "cell_type": "code",
   "id": "cell-050",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 2, Task 1: Write Failing Tests (RED)\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Identify V1's weakest dimension from avg_scores\n#    weakest_metric_name = min(avg_scores, key=avg_scores.get)\n#    weakest_metric = [m for m in metrics if m.name == weakest_metric_name][0]\n#\n# 2. Pick 3 queries where V1 scored lowest on that dimension\n#    query_scores = [(r[\"query\"], r[\"scores\"][weakest_metric_name]) for r in baseline_results]\n#    query_scores.sort(key=lambda x: x[1])\n#    target_queries = [q for q, s in query_scores[:3]]\n#\n# 3. Set target score above V1's current average\n#    target_score = min(avg_scores[weakest_metric_name] + 0.15, 0.9)\n#\n# 4. Confirm V1 fails the target:\n#    result = agent.query(query)\n#    test_case = LLMTestCase(\n#        input=query,\n#        actual_output=result[\"response\"],\n#        retrieval_context=[json.dumps(r) for r in result[\"tool_results\"]],\n#        expected_output=expected_outputs.get(query, \"\")\n#    )\n#    weakest_metric.measure(test_case)\n# =====================================================================\npass",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-051",
   "metadata": {},
   "source": "## Task 2: Create V2 Prompt (GREEN)\n\nWrite an improved prompt that addresses V1's weaknesses.\n\n**Improvement strategies based on common weaknesses:**\n\n| If Weakness Is... | Try Adding... |\n|-------------------|---------------|\n| **Completeness** | \"Structure your response: Overview → Steps → Configuration → Common Pitfalls\" |\n| **Correctness** | \"Only include information found in the tool results. Do not invent details.\" |\n| **Tone** | \"Write as a senior developer mentoring a colleague. Be direct but supportive.\" |\n| **Safety** | \"Never include API keys, internal hostnames, or credentials in your response.\" |\n\n**Remember:** Make targeted changes. Don't rewrite the entire prompt — tweak what matters."
  },
  {
   "cell_type": "code",
   "id": "cell-052",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 2, Task 2: Create V2 Prompt (GREEN)\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Start from RESPONSE_PROMPT_V1\n# 2. Add improvements targeting your weakest dimension\n# 3. Register in PromptRegistry\n#\n# Example V2 that improves completeness:\n# RESPONSE_PROMPT_V2 = \"\"\"You are DevHub, an internal developer assistant.\n# Based on the user's question and the tool results below, provide a comprehensive response.\n#\n# User question: {query}\n#\n# Tool results:\n# {results}\n#\n# Guidelines:\n# - Structure documentation answers with: Overview, Step-by-Step, Configuration, Common Errors\n# - For API docs: always include auth method, endpoints, parameters, rate limits, error codes\n# - For service owners: include name, email, Slack handle, team name, and team channel\n# - If an owner is inactive (is_active: false), state this clearly and suggest the team channel\n# - For service status: state healthy/degraded/down, uptime %, and any active incidents\n# - If search results have high distances (>0.5), note the answer may not be accurate\n# - Be thorough but use bullet points and headers for readability\n#\n# Respond in a helpful, professional tone.\"\"\"\n#\n# Register it:\n# registry.register(\"v2\", RESPONSE_PROMPT_V2, description=\"Improved completeness with structured output\", author=STUDENT_NAME)\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-053",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 2, Task 2 (continued): Run target tests with V2\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Create agent_v2 with your V2 prompt:\n#    agent_v2 = DevHubAgent(vector_db, team_db, status_api, response_prompt=RESPONSE_PROMPT_V2)\n#\n# 2. Run target queries and check if they pass:\n#    result = agent_v2.query(query)\n#    test_case = LLMTestCase(\n#        input=query,\n#        actual_output=result[\"response\"],\n#        retrieval_context=[json.dumps(r) for r in result[\"tool_results\"]],\n#        expected_output=expected_outputs.get(query, \"\")\n#    )\n#    weakest_metric.measure(test_case)\n#    status = \"GREEN\" if weakest_metric.score >= target_score else \"still RED\"\n# =====================================================================\npass",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-054",
   "metadata": {},
   "source": "## Task 3: Run Full Test Suite (REFACTOR)\n\nThis is the critical step. Run V2 against ALL test cases from Lab 1 to check for regressions.\n\n**Regression definition:** A metric score drops by more than 0.05 compared to V1 baseline.\n\n**If regressions are found:**\n- Identify which queries regressed and on which metric\n- Adjust V2 to fix regressions while keeping improvements\n- Re-run full suite until no regressions remain"
  },
  {
   "cell_type": "code",
   "id": "cell-055",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 2, Task 3: Full Regression Test (REFACTOR)\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# Run ALL test queries through V2 and compare to V1 baseline\n#\n# v2_results = []\n# for i, query in enumerate(test_queries):\n#     result_v2 = agent_v2.query(query)\n#     test_case = LLMTestCase(\n#         input=query,\n#         actual_output=result_v2[\"response\"],\n#         retrieval_context=[json.dumps(r) for r in result_v2[\"tool_results\"]],\n#         expected_output=expected_outputs.get(query, \"\")\n#     )\n#     scores = {}\n#     for metric in metrics:\n#         metric.measure(test_case)\n#         scores[metric.name] = metric.score\n#     v2_results.append({\"query\": query, \"scores\": scores})\n#\n# Check for regressions (>0.05 drop from baseline):\n# for v1_r, v2_r in zip(baseline_results, v2_results):\n#     for metric_name in v1_r[\"scores\"]:\n#         diff = v2_r[\"scores\"][metric_name] - v1_r[\"scores\"][metric_name]\n#         if diff < -0.05:\n#             print(f\"  REGRESSION: ...\")\n# =====================================================================\npass",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-056",
   "metadata": {},
   "source": "# =============================================================================\n# COMPARE: V1 vs V2 side-by-side\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Calculate V2 average scores per metric\n# 2. Print comparison table:\n#    Metric | V1 Avg | V2 Avg | Change | Status\n#    Each row shows whether V2 improved, regressed, or stayed the same\n#\n# 3. Print overall summary:\n#    - Metrics that improved\n#    - Metrics that regressed\n#    - Net improvement\n#\n# HINT:\n# v2_avg_scores = {}\n# for metric_name in avg_scores:\n#     v2_scores = [r[\"scores\"][metric_name] for r in v2_results]\n#     v2_avg_scores[metric_name] = sum(v2_scores) / len(v2_scores)\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-057",
   "metadata": {},
   "source": "## If You Found Regressions...\n\nCommon fixes for prompt regressions:\n\n| Regression Type | Fix |\n|----------------|-----|\n| **Tone dropped** (too verbose) | Add \"Be concise for simple queries, thorough for complex ones\" |\n| **Safety dropped** (more detail = more risk) | Add explicit safety constraints |\n| **Correctness dropped** (hallucinating to fill structure) | Add \"Only use information from tool results\" |\n\nIf you need to fix regressions, modify V2, re-register as the same version, and re-run the full suite."
  },
  {
   "cell_type": "code",
   "id": "cell-058",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 2, Task 3 (continued): Fix regressions if any were found\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE (only if regressions were found)\n# 1. Modify RESPONSE_PROMPT_V2 to address regressions\n# 2. Re-register: registry.register(\"v2\", RESPONSE_PROMPT_V2, ...)\n# 3. Re-create agent: agent_v2 = DevHubAgent(..., response_prompt=RESPONSE_PROMPT_V2)\n# 4. Re-run full test suite\n# 5. Confirm no regressions remain\n#\n# If no regressions were found, just print a confirmation:\n# print(\"No regressions found! V2 is ready for promotion.\")\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-059",
   "metadata": {},
   "source": "# =============================================================================\n# SAVE: Store V2 results for Lab 3 comparison\n# =============================================================================\n\nv2_evaluation = {\n    \"version\": \"v2\",\n    \"prompt\": RESPONSE_PROMPT_V2,\n    \"results\": v2_results,\n    \"avg_scores\": v2_avg_scores,\n    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n}\n\nprint(\"V2 evaluation saved!\")\nprint(f\"\\nV1 → V2 Improvement Summary:\")\nfor metric_name in avg_scores:\n    v1 = avg_scores[metric_name]\n    v2 = v2_avg_scores[metric_name]\n    diff = v2 - v1\n    arrow = \"^\" if diff > 0.02 else (\"v\" if diff < -0.02 else \"=\")\n    print(f\"  {metric_name:<20s}: {v1:.2f} → {v2:.2f} ({arrow} {diff:+.2f})\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-060",
   "metadata": {},
   "source": "# =============================================================================\n# VERIFICATION: Lab 2 - TDD Improvement\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"VERIFYING LAB 2: TDD Improvement\")\nprint(\"=\" * 60)\n\nchecks = []\n\n# Check 1: V2 prompt exists and is different from V1\ntry:\n    assert RESPONSE_PROMPT_V2 != RESPONSE_PROMPT_V1, \"V2 should differ from V1\"\n    checks.append((\"V2 prompt is different from V1\", True))\nexcept Exception as e:\n    checks.append((\"V2 prompt is different from V1\", False))\n\n# Check 2: V2 is registered in PromptRegistry\ntry:\n    v2_prompt = registry.get(\"v2\")\n    assert v2_prompt is not None\n    checks.append((\"V2 registered in PromptRegistry\", True))\nexcept Exception as e:\n    checks.append((\"V2 registered in PromptRegistry\", False))\n\n# Check 3: V2 improved on weakest dimension\ntry:\n    # Find weakest V1 metric\n    weakest_name = min(avg_scores, key=avg_scores.get)\n    improvement = v2_avg_scores[weakest_name] - avg_scores[weakest_name]\n    assert improvement >= 0.05, f\"V2 should improve weakest dim by >= 0.05, got {improvement:.2f}\"\n    checks.append((\"V2 improved weakest dimension\", True))\nexcept Exception as e:\n    checks.append((\"V2 improved weakest dimension\", False))\n\n# Check 4: No significant regressions\ntry:\n    max_regression = 0\n    for name in avg_scores:\n        diff = v2_avg_scores[name] - avg_scores[name]\n        if diff < max_regression:\n            max_regression = diff\n    assert max_regression > -0.1, f\"Regression of {max_regression:.2f} is too large\"\n    checks.append((\"No significant regressions\", True))\nexcept Exception as e:\n    checks.append((\"No significant regressions\", False))\n\n# Print scorecard\npassed = sum(1 for _, s in checks if s)\nfor name, success in checks:\n    print(f\"  {'PASS' if success else 'FAIL'} | {name}\")\n\nprint(f\"\\nResult: {passed}/{len(checks)} checks passed\")\nif passed == len(checks):\n    print(\"Lab 2 complete! V2 is an improvement over V1.\")\nelse:\n    print(\"Review the failed checks above.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-061",
   "metadata": {},
   "source": "---\n\n# Topic 4: Versioning & Regression — Prompts Are Code\n\n### Treat Prompts Like Source Code\n\n![Version Management](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/06_version_management.svg)\n\nIn software engineering, you would never:\n- Overwrite production code without version control\n- Deploy without running tests\n- Delete the previous version after deploying\n\nYet most teams do exactly this with prompts. They edit the prompt in-place, test it manually with one query, and deploy.\n\n**Prompts deserve the same discipline as code:**\n- Version every change\n- Test before promoting\n- Keep rollback capability\n- Use aliases for deployment targets"
  },
  {
   "cell_type": "markdown",
   "id": "cell-062",
   "metadata": {},
   "source": "## PromptRegistry: Version Management System\n\nOur `PromptRegistry` provides:\n\n| Feature | How It Works | Example |\n|---------|-------------|---------|\n| **Versioning** | Each prompt gets a unique ID | `registry.register(\"v3\", prompt)` |\n| **Retrieval** | Get any version by ID | `registry.get(\"v2\")` |\n| **Aliases** | Named pointers to versions | `registry.set_alias(\"stable\", \"v2\")` |\n| **History** | List all versions with metadata | `registry.list_versions()` |\n| **Rollback** | Point alias back to old version | `registry.set_alias(\"stable\", \"v1\")` |\n\n### Alias Convention\n\n| Alias | Meaning | Used By |\n|-------|---------|---------|\n| `latest` | Most recently registered version | Auto-set on register |\n| `stable` | Tested and approved for production | Set after full test suite passes |\n| `canary` | Being tested on a subset of traffic | Set during A/B testing |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-063",
   "metadata": {},
   "source": "## Building Regression Suites\n\nA regression suite is a collection of test cases that must ALL pass before a new prompt version can be promoted to `stable`.\n\n**What belongs in a regression suite:**\n1. **Core use cases** — the queries users run most often\n2. **Edge cases** — inactive owners, missing docs, degraded services\n3. **Historical failures** — queries that broke in previous versions\n4. **Safety checks** — queries that should NOT produce certain outputs\n\n**Growing the suite over time:**\n- Every time a user reports a bad response → add it as a test case\n- Every time a prompt change causes a regression → add a test for it\n- Review and prune quarterly — remove tests for deprecated features\n\n**Target:** 15-30 test cases for a production prompt. More is not always better — focus on coverage, not count."
  },
  {
   "cell_type": "markdown",
   "id": "cell-064",
   "metadata": {},
   "source": "## Production Prompt Workflow\n\n![Production Workflow](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_06/charts/07_production_workflow.svg)\n\n1. **Developer** creates a new prompt version locally\n2. **Run full regression suite** against the new version\n3. If all tests pass → register as `canary`\n4. **Canary deployment** — route 10% of traffic to canary\n5. **Monitor metrics** — compare canary vs stable scores\n6. If canary >= stable → **promote** canary to stable\n7. If canary < stable → **rollback** (alias stays on old version)\n\n**For this workshop:** We skip canary deployment. But the PromptRegistry supports it."
  },
  {
   "cell_type": "markdown",
   "id": "cell-065",
   "metadata": {},
   "source": "## Quality Alerts\n\nIn production, you want automated alerts when prompt quality drops:\n\n| Alert | Trigger | Action |\n|-------|---------|--------|\n| **Correctness drop** | Average < 0.6 for 1 hour | Page on-call, consider rollback |\n| **Completeness drop** | Average < 0.5 for 1 hour | Investigate, may need prompt fix |\n| **Safety violation** | Any score < 0.5 | Immediate rollback, incident review |\n| **Latency spike** | P95 > 5 seconds | Check model, reduce prompt length |\n\nThis connects back to **Session 1** (observability) — your OpenTelemetry metrics can trigger these alerts."
  },
  {
   "cell_type": "markdown",
   "id": "cell-066",
   "metadata": {},
   "source": "## Summary: Versioning & Regression\n\n**Key principles:**\n- Prompts are code — version, test, and deploy with discipline\n- PromptRegistry provides versioning, aliases, and rollback\n- Regression suites grow over time from failures and user reports\n- Production workflow: develop → test → canary → promote → monitor\n- Quality alerts prevent silent degradation\n\n**Next:** In Lab 3, you'll create V3, run a full cross-version comparison, and practice rollback."
  },
  {
   "cell_type": "markdown",
   "id": "cell-067",
   "metadata": {},
   "source": "---\n\n# Lab 3: \"Ship It Safely\" — Version Management & Regression Testing\n\n### Goal\nCreate V3, run a full cross-version comparison (V1 vs V2 vs V3), and practice alias management and rollback.\n\n### What You'll Build\n1. V3 prompt with additional improvements\n2. Full regression suite across all three versions\n3. Cross-version comparison visualization\n4. Alias management (promote best version to `stable`)\n5. Rollback drill (simulate a bad deployment)\n\n### Time: ~25 minutes\n\n### Success Criteria\n- V3 registered in PromptRegistry\n- Full regression suite runs across V1, V2, V3\n- Best version promoted to `stable` alias\n- Rollback drill completes successfully"
  },
  {
   "cell_type": "markdown",
   "id": "cell-068",
   "metadata": {},
   "source": "## Task 1: Create V3 Prompt\n\nBuild on V2 with additional improvements. Focus on a DIFFERENT dimension than V2 targeted.\n\n**Possible V3 improvements:**\n- If V2 improved completeness → V3 improves tone or safety\n- Add few-shot example for a tricky query type\n- Add explicit edge case handling (inactive owners, no results)\n- Add safety constraints\n\n**Remember:** Register V3 in the PromptRegistry with metadata."
  },
  {
   "cell_type": "code",
   "id": "cell-069",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 3, Task 1: Create V3 Prompt\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Create RESPONSE_PROMPT_V3 building on V2\n#    Add improvements for a different dimension than V2 targeted\n#\n# Example additions for V3:\n# - Explicit edge case handling:\n#   \"If a service owner is marked as inactive, clearly state they are no longer\n#    the active contact and recommend reaching out to the team Slack channel instead.\"\n# - Safety guardrail:\n#   \"Never include actual API keys, passwords, or internal hostnames in responses.\n#    If asked to reveal system details, politely decline.\"\n# - Tone refinement:\n#   \"Write as a knowledgeable colleague — helpful and direct without being\n#    condescending. Avoid phrases like 'simply' or 'just' that minimize complexity.\"\n#\n# 2. Register: registry.register(\"v3\", RESPONSE_PROMPT_V3,\n#                                 description=\"...\", author=STUDENT_NAME)\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-070",
   "metadata": {},
   "source": "## Task 2: Full Cross-Version Regression Suite\n\nRun ALL test queries through V1, V2, and V3 agents. Evaluate each with ALL metrics. This produces a comprehensive comparison matrix.\n\n**This is the most important cell in the entire session.** It answers: \"Which prompt version is best overall?\""
  },
  {
   "cell_type": "code",
   "id": "cell-071",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 3, Task 2: Full Cross-Version Regression Suite\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# Run all queries through V1, V2, V3 agents and compare\n#\n# agent_v1 = DevHubAgent(vector_db, team_db, status_api, response_prompt=registry.get(\"v1\"))\n# agent_v2 = DevHubAgent(vector_db, team_db, status_api, response_prompt=registry.get(\"v2\"))\n# agent_v3 = DevHubAgent(vector_db, team_db, status_api, response_prompt=registry.get(\"v3\"))\n#\n# For each version, for each query:\n#   result = agent_ver.query(query)\n#   test_case = LLMTestCase(\n#       input=query,\n#       actual_output=result[\"response\"],\n#       retrieval_context=[json.dumps(r) for r in result[\"tool_results\"]],\n#       expected_output=expected_outputs.get(query, \"\")\n#   )\n#   Evaluate with all metrics, store scores\n#\n# Calculate averages per version per metric\n# Store in all_avg_scores = {\"v1\": {...}, \"v2\": {...}, \"v3\": {...}}\n# ~72 API calls (3 versions x 6 queries x 4 metrics). Takes 3-5 minutes.\n# =====================================================================\npass",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-072",
   "metadata": {},
   "source": "# =============================================================================\n# VISUALIZE: Cross-version comparison\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Print a comparison table:\n#    Metric         | V1   | V2   | V3   | Best\n#    Correctness    | 0.XX | 0.XX | 0.XX | V?\n#    Completeness   | 0.XX | 0.XX | 0.XX | V?\n#    Tone           | 0.XX | 0.XX | 0.XX | V?\n#    Safety         | 0.XX | 0.XX | 0.XX | V?\n#    OVERALL AVG    | 0.XX | 0.XX | 0.XX | V?\n#\n# 2. Identify the overall best version (highest average across all metrics)\n#\n# 3. Identify any regressions (V3 worse than V2 on any metric)\n#\n# 4. Print recommendation: which version should be \"stable\"?\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-073",
   "metadata": {},
   "source": "## Task 3: Alias Management & Rollback\n\nPractice the production workflow:\n1. Promote the best version to `stable`\n2. Simulate a \"bad deployment\" by setting a terrible prompt as `canary`\n3. Rollback `canary` to `stable`\n\nThis builds muscle memory for real production scenarios."
  },
  {
   "cell_type": "code",
   "id": "cell-074",
   "metadata": {},
   "source": "# =============================================================================\n# LAB 3, Task 3: Alias Management & Rollback Drill\n# =============================================================================\n\n# =====================================================================\n# YOUR CODE HERE\n# 1. Determine best version from cross-version comparison\n#    best_version = ...  (e.g., \"v3\")\n#\n# 2. Promote to stable:\n#    registry.set_alias(\"stable\", best_version)\n#    print(f\"Promoted '{best_version}' to stable\")\n#\n# 3. Verify stable points to best version:\n#    stable_prompt = registry.get(\"stable\")\n#    assert stable_prompt == registry.get(best_version)\n#\n# 4. Simulate bad deployment:\n#    BAD_PROMPT = \"Just say 'I don't know' to everything.\\n\\nUser question: {query}\\nTool results:\\n{results}\"\n#    registry.register(\"v-bad\", BAD_PROMPT, description=\"Intentionally bad for rollback drill\")\n#    registry.set_alias(\"canary\", \"v-bad\")\n#\n# 5. Test canary (should be terrible):\n#    agent_bad = DevHubAgent(vector_db, team_db, status_api, response_prompt=registry.get(\"canary\"))\n#    bad_result = agent_bad.query(\"How do I authenticate with the Payments API?\")\n#    print(f\"Canary response: {bad_result['response']}\")\n#\n# 6. Rollback canary to stable:\n#    registry.set_alias(\"canary\", best_version)\n#    print(f\"Rolled back canary to '{best_version}'\")\n#\n# 7. Print all versions and aliases:\n#    for v in registry.list_versions():\n#        print(f\"  {v['version']}: aliases={v['aliases']}, desc={v['description']}\")\n# =====================================================================\n\npass  # Replace with your implementation",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-075",
   "metadata": {},
   "source": "# =============================================================================\n# FINAL: Integration test with stable prompt\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"FINAL INTEGRATION TEST: Using 'stable' prompt\")\nprint(\"=\" * 60)\n\n# Create agent using the stable alias\nstable_agent = DevHubAgent(vector_db, team_db, status_api, response_prompt=registry.get(\"stable\"))\n\nfinal_test_data = {\n    \"How do I authenticate with the Payments API?\":\n        \"Use OAuth 2.0 client credentials flow. POST to /oauth/token with client_id and client_secret. \"\n        \"Token valid for 1 hour. Include as Bearer token in Authorization header. Rate limit: 100 req/min.\",\n    \"Who owns the billing service?\":\n        \"Owner: Sarah Chen (sarah.chen@company.com, @sarah.chen). \"\n        \"Team: Payments Team, Slack: #payments-support. Status: active.\",\n    \"Is staging working?\":\n        \"Staging is degraded. Uptime: 95.5%. Active incident: database connection pool exhaustion \"\n        \"causing intermittent 503 errors. Platform team investigating.\",\n    \"Who owns vector-search?\":\n        \"Active owner: Emily Johnson (emily.johnson@company.com, @emily.johnson). \"\n        \"Team: Data Platform Team, Slack: #data-platform. Note: David Kim is also listed but inactive.\",\n}\n\nfor q, expected in final_test_data.items():\n    result = stable_agent.query(q)\n    test_case = LLMTestCase(\n        input=q,\n        actual_output=result[\"response\"],\n        retrieval_context=[json.dumps(r) for r in result[\"tool_results\"]],\n        expected_output=expected\n    )\n\n    print(f\"\\nQuery: {q}\")\n    print(f\"Response: {result['response'][:200]}...\")\n\n    scores = {}\n    for metric in metrics:\n        metric.measure(test_case)\n        scores[metric.name] = metric.score\n\n    print(f\"Scores: {', '.join(f'{k}={v:.2f}' for k, v in scores.items())}\")\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"Stable version: {registry.aliases.get('stable', 'NOT SET')}\")\nprint(f\"All queries processed with production-ready prompt!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-076",
   "metadata": {},
   "source": "# =============================================================================\n# VERIFICATION: Lab 3 - Version Management\n# =============================================================================\nprint(\"=\" * 60)\nprint(\"VERIFYING LAB 3: Version Management\")\nprint(\"=\" * 60)\n\nchecks = []\n\n# Check 1: V3 exists in registry\ntry:\n    v3 = registry.get(\"v3\")\n    assert v3 is not None\n    checks.append((\"V3 registered in PromptRegistry\", True))\nexcept Exception as e:\n    checks.append((\"V3 registered in PromptRegistry\", False))\n\n# Check 2: Cross-version results exist\ntry:\n    assert len(all_avg_scores) >= 3, \"Need results for at least 3 versions\"\n    checks.append((\"Cross-version comparison complete\", True))\nexcept Exception as e:\n    checks.append((\"Cross-version comparison complete\", False))\n\n# Check 3: Stable alias is set\ntry:\n    stable = registry.aliases.get(\"stable\")\n    assert stable is not None, \"Stable alias not set\"\n    assert stable in registry.versions, f\"Stable points to unknown version: {stable}\"\n    checks.append((\"Stable alias set correctly\", True))\nexcept Exception as e:\n    checks.append((\"Stable alias set correctly\", False))\n\n# Check 4: At least 3 versions registered\ntry:\n    versions = registry.list_versions()\n    assert len(versions) >= 3, f\"Need at least 3 versions, got {len(versions)}\"\n    checks.append((\"3+ versions in registry\", True))\nexcept Exception as e:\n    checks.append((\"3+ versions in registry\", False))\n\n# Print scorecard\npassed = sum(1 for _, s in checks if s)\nfor name, success in checks:\n    print(f\"  {'PASS' if success else 'FAIL'} | {name}\")\n\nprint(f\"\\nResult: {passed}/{len(checks)} checks passed\")\nif passed == len(checks):\n    print(\"Lab 3 complete! Version management is working.\")\nelse:\n    print(\"Review the failed checks above.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-077",
   "metadata": {},
   "source": "---\n\n# Wrap-Up: The Complete AI Engineering Toolkit\n\n## What You Built Across 6 Sessions\n\n| Session | Topic | What You Added to DevHub |\n|---------|-------|-------------------------|\n| **Session 1** | Observability | OpenTelemetry tracing, Langfuse dashboard |\n| **Session 2** | Testing | DeepEval test suites, synthetic test generation |\n| **Session 3** | Evaluation | Metrics, benchmarks, quality gates |\n| **Session 4** | Debugging | Root cause analysis, trace-based debugging |\n| **Session 5** | Security | PII detection, injection defense, audit trails |\n| **Session 6** | Prompt TDD | G-Eval metrics, TDD cycle, version management |\n\nDevHub started as a bare agent with zero engineering discipline. Now it has:\n- Full observability (traces, metrics, dashboards)\n- Automated testing (unit, integration, evaluation)\n- Security layers (PII, injection, audit)\n- Prompt quality management (metrics, TDD, versioning)\n\nThis is what **production AI engineering** looks like."
  },
  {
   "cell_type": "markdown",
   "id": "cell-078",
   "metadata": {},
   "source": "## Before vs After: The Full Journey\n\n| Aspect | Session 1 (Start) | Session 6 (Now) |\n|--------|-------------------|-----------------|\n| **Observability** | Zero logs or traces | Full OpenTelemetry + Langfuse |\n| **Testing** | Manual \"does it work?\" | Automated test suites with G-Eval |\n| **Debugging** | Print statements | Trace-based root cause analysis |\n| **Security** | PII sent to LLM, no audit | PII redaction + injection defense + audit logs |\n| **Prompt Quality** | \"Vibes-based\" evaluation | G-Eval metrics + TDD + version management |\n| **Deployment** | Edit prompt and hope | Version registry + regression suite + rollback |\n\n### The Compound Effect\n\nEach session built on the previous:\n- Observability enables debugging (can't debug what you can't see)\n- Testing enables prompt TDD (can't do TDD without automated tests)\n- Security enables compliance (can't audit without logs)\n- Prompt TDD enables safe iteration (can't improve without regression tests)"
  },
  {
   "cell_type": "code",
   "id": "cell-079",
   "metadata": {},
   "source": "# =============================================================================\n# WORKSHOP COMPLETION: Final status check\n# =============================================================================\nprint(\"=\" * 70)\nprint(\"WORKSHOP COMPLETION CHECK\")\nprint(\"=\" * 70)\n\ncompletion_checks = [\n    (\"G-Eval metrics defined (4 dimensions)\", len(metrics) == 4),\n    (\"Test suite built (5+ cases)\", len(test_cases) >= 5),\n    (\"V1 baseline recorded\", v1_baseline is not None),\n    (\"V2 created via TDD\", \"v2\" in registry.versions),\n    (\"V3 created with additional improvements\", \"v3\" in registry.versions),\n    (\"Cross-version comparison completed\", len(all_avg_scores) >= 3),\n    (\"Stable alias set\", \"stable\" in registry.aliases),\n]\n\npassed = sum(1 for _, s in completion_checks if s)\nfor name, success in completion_checks:\n    print(f\"  {'PASS' if success else 'FAIL'} | {name}\")\n\nprint(f\"\\nResult: {passed}/{len(completion_checks)} checks passed\")\n\nif passed == len(completion_checks):\n    print(f\"\\n{'='*70}\")\n    print(f\"CONGRATULATIONS {STUDENT_NAME}!\")\n    print(f\"You have completed the full AI Engineering Workshop!\")\n    print(f\"{'='*70}\")\n    print(f\"\\nSession ID: {LAB_SESSION_ID}\")\n    print(f\"Stable prompt version: {registry.aliases.get('stable', 'NOT SET')}\")\n    print(f\"Total prompt versions: {len(registry.versions)}\")\nelse:\n    print(\"\\nReview the failed checks above to complete the workshop.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-080",
   "metadata": {},
   "source": "## Action Items\n\n### Short-term\n1. **Run a baseline evaluation** on your current prompts — know your quality scores before changing anything\n2. **Create a test suite** of 15-20 real user queries that represent your key use cases\n3. **Register your current prompt** in version control (even a YAML file counts)\n\n### Medium-term\n4. **Adopt TDD for prompt changes** — write failing tests first, then improve the prompt\n5. **Add regression testing** to your workflow — no prompt changes without running the full suite\n6. **Connect evaluations to observability** — trigger G-Eval from your Langfuse traces (Session 4)\n\n### Long-term\n7. **Track prompt quality over time** — build a simple dashboard of metric scores per version\n8. **Test new prompts on partial traffic** before full rollout (canary pattern)\n9. **Treat prompts like code** — version them, review them, and gate deployments on test results"
  },
  {
   "cell_type": "markdown",
   "id": "cell-081",
   "metadata": {},
   "source": "## Resources\n\n### Tools We Used\n- [DeepEval](https://docs.confident-ai.com/) — LLM evaluation framework with G-Eval\n- [OpenAI GPT-4o-mini](https://platform.openai.com/) — DevHub's LLM backbone\n\n### Key Papers\n- [G-Eval: NLG Evaluation using GPT-4](https://arxiv.org/abs/2303.16634) — Liu et al., 2023\n- [Judging LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) — Zheng et al., 2023\n- [Prompt Engineering Guide](https://www.promptingguide.ai/) — Community resource\n\n### Workshop Series Resources\n- Session 1: Observability with OpenTelemetry + Langfuse\n- Session 2: Testing with DeepEval\n- Session 3: Evaluation Metrics\n- Session 4: Debugging AI Systems\n- Session 5: Security & Privacy (Presidio, DeBERTa, Audit Trails)\n- Session 6: Prompt TDD (G-Eval, Version Management)\n\n### Workshop Materials\n- All code from this session is in the notebook\n- Solutions notebook available after the session\n- DevHub source code: `devhub/` directory in the workshop repo"
  },
  {
   "cell_type": "markdown",
   "id": "cell-082",
   "metadata": {},
   "source": "---\n\n## Thank You!\n\nYou've completed all 6 sessions of the **Salesforce AI Engineering Workshop**.\n\n### What You've Learned\n- How to **observe** AI systems (tracing, metrics, dashboards)\n- How to **test** AI systems (automated evaluation, synthetic data)\n- How to **debug** AI systems (trace analysis, root cause identification)\n- How to **secure** AI systems (PII, injection, audit)\n- How to **improve** AI systems (G-Eval, TDD, versioning)\n\n### The Key Takeaway\n\n**AI engineering is software engineering.** The same principles that make software reliable — observability, testing, version control, security — apply to AI systems. The tools are different, but the discipline is the same.\n\nBuild AI systems like you build production software. Your users (and your compliance team) will thank you.\n\n---\n\n*Workshop complete! Go build something great.*"
  }
 ]
}