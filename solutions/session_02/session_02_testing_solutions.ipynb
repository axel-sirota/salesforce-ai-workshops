{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Testing Strategies for AI Applications\n",
    "\n",
    "**Salesforce AI Workshop Series**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Understand why** traditional testing approaches fail for LLM applications\n",
    "2. **Build automated evaluation harnesses** that run 100 tests in 3 minutes\n",
    "3. **Implement G-Eval scoring** with custom criteria and multi-dimensional quality assessment\n",
    "4. **Generate synthetic test datasets** with paraphrases and adversarial inputs\n",
    "5. **Catch regressions** before they reach production through systematic testing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python knowledge (functions, classes, decorators)\n",
    "- Session 1 completed (understanding of DevHub architecture and observability)\n",
    "- No prior AI testing experience required\n",
    "\n",
    "## Session Format\n",
    "\n",
    "- **~2.5 hours hands-on**\n",
    "- Instructor demos followed by your labs\n",
    "- All code runs in this notebook\n",
    "- We'll test the DevHub application from Session 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: \"The New Prompt Broke Everything\"\n",
    "\n",
    "Picture this scenario...\n",
    "\n",
    "You manually test 20 prompt variations over **2 days**. Everything looks good. You deploy the new prompt to production.\n",
    "\n",
    "**2 hours later**, your Slack explodes:\n",
    "\n",
    "> \"The refund responses are completely wrong now!\" - @support-lead\n",
    "> \"Order status queries are returning gibberish\" - @customer-success\n",
    "> \"Did someone change something??\" - @engineering-manager\n",
    "\n",
    "You check your manual test results. All 20 tests passed! But those tests were all about **refunds**. Nobody tested order status. Nobody tested shipping queries. Nobody tested edge cases.\n",
    "\n",
    "**The new prompt fixed refunds but broke everything else.**\n",
    "\n",
    "You have NO IDEA:\n",
    "- How many features are actually broken\n",
    "- Which prompt change caused the regression\n",
    "- How to prevent this from happening again\n",
    "\n",
    "**This session teaches you how to NEVER be in this situation again.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We'll Build Today\n",
    "\n",
    "1. **Understand** why LLM outputs can't be tested like traditional software\n",
    "2. **Learn** DeepEval - the pytest-like framework for LLM testing\n",
    "3. **Master** G-Eval - the state-of-the-art LLM-as-judge approach\n",
    "4. **Build** an automated evaluation harness for DevHub (Lab 1)\n",
    "5. **Generate** synthetic test data and run regression tests (Lab 2)\n",
    "\n",
    "---\n",
    "\n",
    "## Google Colab Setup\n",
    "\n",
    "If you're running this in Google Colab:\n",
    "\n",
    "1. **Runtime → Change runtime type → Python 3**\n",
    "2. No GPU needed for this session\n",
    "3. All data is loaded from this notebook\n",
    "\n",
    "Let's start by installing the required packages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# INSTALL REQUIRED PACKAGES\n# =============================================================================\n# Run this cell first! It installs all dependencies needed for this session.\n# This may take 1-2 minutes on first run.\n\n!pip install -q \\\n    deepeval>=1.0.0 \\\n    openai>=1.0.0 \\\n    chromadb>=0.4.0 \\\n    pandas>=2.0.0 \\\n    rich>=13.0.0 \\\n    opentelemetry-api>=1.20.0 \\\n    opentelemetry-sdk>=1.20.0 \\\n    langchain-core \\\n    langchain-community \\\n    langchain-text-splitters\n\nprint(\"All packages installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "# These credentials are needed for LLM-as-judge evaluation.\n",
    "\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# OpenAI Configuration (for G-Eval metrics)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Your instructor will provide this key\n",
    "OPENAI_API_KEY = \"sk-...\"  # INSTRUCTOR: Fill this before class\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Student Identity\n",
    "# -----------------------------------------------------------------------------\n",
    "# Change this to your name (lowercase, no spaces)\n",
    "# This helps identify your test results\n",
    "STUDENT_NAME = \"your-name-here\"  # Example: \"john-smith\"\n",
    "\n",
    "# Validate student name\n",
    "if STUDENT_NAME == \"your-name-here\" or \" \" in STUDENT_NAME:\n",
    "    print(\"ERROR: Please set STUDENT_NAME to your name (lowercase, no spaces)\")\n",
    "    print(\"   Example: STUDENT_NAME = 'john-smith'\")\n",
    "else:\n",
    "    print(f\"Student identity set: {STUDENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TEST OPENAI CONNECTION\n# =============================================================================\n# G-Eval metrics use OpenAI for LLM-as-judge evaluation.\n\nfrom openai import OpenAI\n\ntry:\n    client = OpenAI()\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Say 'Testing ready!' in exactly 2 words\"}],\n        max_tokens=10\n    )\n\n    result = response.choices[0].message.content\n    print(f\"OpenAI connection successful!\")\n    print(f\"   Response: {result}\")\n\nexcept Exception as e:\n    print(f\"OpenAI connection failed: {e}\")\n    print(\"   Check that OPENAI_API_KEY is set correctly\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TEST DEEPEVAL INSTALLATION\n# =============================================================================\n# Verify DeepEval is installed and working.\n\ntry:\n    from deepeval import evaluate\n    from deepeval.metrics import GEval\n    from deepeval.test_case import LLMTestCase\n\n    print(\"DeepEval imported successfully!\")\n    print(\"   Available: evaluate, GEval, LLMTestCase\")\n\nexcept ImportError as e:\n    print(f\"DeepEval import failed: {e}\")\n    print(\"   Try running the install cell again\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Setup Complete!\n\nIf you see success messages above for:\n- Packages installed\n- Student identity set\n- OpenAI connection\n- DeepEval imported\n\n**You're ready to begin!**\n\nIf any step failed, raise your hand or message in the workshop chat.\n\n---\n\n**Next:** We'll understand why traditional testing doesn't work for LLMs.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Topic 1: Why Traditional Testing Fails for LLMs\n\nBefore we can fix the problem, we need to understand it.\n\nTraditional software testing works because outputs are **deterministic**. Given the same input, you always get the same output. LLMs break this assumption completely.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## The Three Fundamental Challenges\n\n### Challenge 1: Non-Deterministic Outputs\n\nThe same prompt can produce different responses:\n\n```python\n# Traditional software\nassert add(2, 3) == 5  # Always true\n\n# LLM output\nresponse = llm(\"What is 2+3?\")\n# Could be: \"5\", \"The answer is 5\", \"2+3=5\", \"Five\", etc.\nassert response == \"5\"  # Often fails!\n```\n\n### Challenge 2: Multi-Dimensional Quality\n\nLLM outputs have many quality dimensions:\n- **Correctness**: Is the answer factually right?\n- **Relevance**: Does it answer what was asked?\n- **Coherence**: Is it logically structured?\n- **Fluency**: Is it grammatically correct?\n- **Tone**: Is it appropriately professional?\n\nA response can be correct but incoherent, or fluent but wrong.\n\n### Challenge 3: Scale\n\nManual evaluation doesn't scale:\n- **20 test cases**: 2 days of manual review\n- **100 test cases**: 2 weeks of manual review\n- **1000 test cases**: Impossible\n\n**Only 5% of teams** track LLM quality metrics in production. **81%** don't test prompts before deployment.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Traditional Testing vs LLM Testing\n\n![Traditional vs LLM Testing](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_02/charts/01_traditional_vs_llm_testing.svg)\n\n**Traditional Testing:**\n- Input → Deterministic Function → Expected Output\n- Binary: Pass or Fail\n- Fast: Milliseconds per test\n\n**LLM Testing:**\n- Input → Non-Deterministic LLM → Many Valid Outputs\n- Spectrum: Quality Score (0.0 - 1.0)\n- Slow: Needs LLM judge (seconds per test)\n\n**Key Insight:** We need to test *quality*, not *equality*.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Why assert == Doesn't Work for LLMs\n# =============================================================================\n# Let's see the problem firsthand.\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ndef ask_llm(question: str) -> str:\n    \"\"\"Simple LLM call.\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": question}],\n        temperature=0.7,  # Some randomness\n        max_tokens=50\n    )\n    return response.choices[0].message.content\n\n# Ask the same question 3 times\nquestion = \"What is the capital of France?\"\nexpected = \"Paris\"\n\nprint(f\"Question: {question}\")\nprint(f\"Expected: '{expected}'\")\nprint(\"-\" * 50)\n\nfor i in range(3):\n    response = ask_llm(question)\n    matches = response.strip() == expected\n    print(f\"Response {i+1}: '{response}'\")\n    print(f\"   Exact match: {matches}\")\n    print()\n\nprint(\"Notice how the responses vary, even though they're all CORRECT!\")\nprint(\"Traditional assert == would fail most of these.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## What We Actually Need\n\nInstead of checking **equality**, we need to check **quality**.\n\n### The LLM-as-Judge Approach\n\nUse another LLM to evaluate the output:\n\n```python\n# Instead of:\nassert response == \"Paris\"\n\n# We need:\nscore = evaluate_with_llm(\n    question=\"What is the capital of France?\",\n    response=\"The capital of France is Paris.\",\n    criteria=\"Is the answer correct?\"\n)\nassert score >= 0.8  # 80% or better\n```\n\n### Why This Works\n\n- LLMs understand **semantics**, not just strings\n- \"Paris\" and \"The capital is Paris\" mean the same thing\n- We can define **custom criteria** for what \"good\" means\n- Research shows LLMs align with humans **81% of the time** - better than humans align with each other!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Quality-Based Testing\n# =============================================================================\n# Let's build a simple quality checker.\n\ndef check_correctness(question: str, response: str, expected_answer: str) -> float:\n    \"\"\"Use LLM to check if response is semantically correct.\"\"\"\n\n    prompt = f\"\"\"Rate how well the response answers the question correctly.\n\nQuestion: {question}\nExpected Answer: {expected_answer}\nActual Response: {response}\n\nScore from 0.0 (completely wrong) to 1.0 (perfectly correct).\nConsider semantic equivalence, not exact wording.\n\nRespond with ONLY a number between 0.0 and 1.0.\"\"\"\n\n    result = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0,\n        max_tokens=10\n    )\n\n    try:\n        score = float(result.choices[0].message.content.strip())\n        return min(max(score, 0.0), 1.0)  # Clamp to 0-1\n    except:\n        return 0.0\n\n# Test it\ntest_cases = [\n    (\"What is the capital of France?\", \"Paris\", \"Paris\"),\n    (\"What is the capital of France?\", \"The capital of France is Paris.\", \"Paris\"),\n    (\"What is the capital of France?\", \"London\", \"Paris\"),\n    (\"What is the capital of France?\", \"Paris is the beautiful capital city of France.\", \"Paris\"),\n]\n\nprint(\"Quality-Based Testing Results:\")\nprint(\"-\" * 60)\n\nfor question, response, expected in test_cases:\n    score = check_correctness(question, response, expected)\n    status = \"PASS\" if score >= 0.8 else \"FAIL\"\n    print(f\"Response: '{response[:40]}...'\")\n    print(f\"   Score: {score:.2f} [{status}]\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## The Cost Problem (And Why DeepEval Exists)\n\nBuilding quality checkers from scratch has problems:\n\n### 1. Prompt Engineering is Hard\n- Getting reliable scores requires careful prompt design\n- Different criteria need different prompts\n- Edge cases break simple prompts\n\n### 2. Scoring is Inconsistent\n- Raw LLM outputs vary\n- Need normalization techniques\n- Token probabilities help but add complexity\n\n### 3. No Standardization\n- Every team builds their own approach\n- Hard to compare results across projects\n- No best practices\n\n**DeepEval solves all of this** with:\n- **50+ research-backed metrics** (G-Eval, Answer Relevancy, Faithfulness, etc.)\n- **Pytest-like interface** (familiar to developers)\n- **Normalization built-in** (consistent scores)\n- **Battle-tested** by thousands of teams",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## The State of LLM Testing (Research Statistics)\n\nFrom Confident AI and industry surveys:\n\n| Statistic | Value | Implication |\n|-----------|-------|-------------|\n| Teams tracking LLM quality in production | **5%** | Most teams are flying blind |\n| Teams that test prompts before deployment | **19%** | 81% deploy without testing |\n| AI initiatives that fail to meet outcomes | **70-85%** | Lack of quality assurance |\n| G-Eval correlation with human judgment | **0.87** | Better than human-human agreement |\n| Automated vs manual testing speed | **100 tests in 3 min** vs **20 tests in 2 days** | 100x faster |\n\n**The opportunity:** Implementing proper testing gives you a massive competitive advantage.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Real-World Example: Gorgias\n\n**Gorgias** is a customer service platform handling millions of support tickets.\n\n### Before Systematic Testing:\n- Product managers manually tested 20 prompt variations\n- Took 2 days per prompt change\n- Still had production incidents from missed edge cases\n\n### After Implementing DeepEval:\n- **100+ test cases** run on every prompt change\n- **3 minutes** per evaluation cycle\n- **Autonomous prompt engineering team** - LLMs improving LLMs\n- Catches regressions **before deployment**\n\n> \"We use DeepEval daily to version control prompts, run evaluations on regression datasets, and review logs.\"\n\nSource: [PromptLayer Case Study](https://www.promptlayer.com)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Key Insight: Test Quality, Not Equality\n\n**Traditional Testing:**\n```python\nassert response == expected_output  # Fails for LLMs\n```\n\n**LLM Testing:**\n```python\nscore = evaluate_quality(response, criteria)\nassert score >= threshold  # Works!\n```\n\n**The shift:**\n- From **exact matching** to **quality scoring**\n- From **binary pass/fail** to **continuous scores**\n- From **manual review** to **automated LLM judges**\n\n---\n\n**Next:** We'll learn DeepEval - the framework that makes this easy.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Topic 2: DeepEval - The Pytest for LLMs\n\nDeepEval is an open-source framework that makes LLM testing as easy as unit testing.\n\nThink of it as **pytest, but for AI outputs**.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Why DeepEval?\n\n### The Problem with Building Your Own\n- Reinventing the wheel for every metric\n- Inconsistent scoring across projects\n- No community best practices\n- Hard to maintain\n\n### What DeepEval Provides\n\n| Feature | Benefit |\n|---------|---------|\n| **50+ metrics** | Research-backed, ready to use |\n| **Pytest integration** | Familiar developer experience |\n| **LLM-as-judge** | Semantic evaluation, not string matching |\n| **Synthetic data** | Generate test cases automatically |\n| **CI/CD ready** | Integrate with GitHub Actions, etc. |\n| **Vendor-neutral** | Works with OpenAI, Anthropic, local models |\n\n### Core Philosophy\n\n> \"You can't improve what you don't measure, and you can't trust what you don't test.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## DeepEval Architecture\n\n![DeepEval Architecture](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_02/charts/02_deepeval_architecture.svg)\n\n**Components:**\n\n1. **Test Cases** (`LLMTestCase`)\n   - Input, actual output, expected output, context\n   - The \"unit\" of LLM testing\n\n2. **Metrics** (G-Eval, AnswerRelevancy, Faithfulness, etc.)\n   - Define what \"good\" means\n   - Each metric produces a score (0-1)\n\n3. **LLM Judge** (GPT-4, Claude, etc.)\n   - Evaluates test cases against metrics\n   - Uses chain-of-thought reasoning\n\n4. **Evaluation Results**\n   - Scores, reasons, pass/fail status\n   - Aggregated reports",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Anatomy of an LLM Test Case\n\nA test case in DeepEval represents a single LLM interaction:\n\n```python\nfrom deepeval.test_case import LLMTestCase\n\ntest_case = LLMTestCase(\n    # REQUIRED\n    input=\"What is the capital of France?\",           # The prompt\n    actual_output=\"Paris is the capital of France.\",  # LLM response\n\n    # OPTIONAL (depending on metric)\n    expected_output=\"Paris\",                          # Ground truth\n    context=[\"France is a country in Europe...\"],    # Retrieved docs\n    retrieval_context=[\"Doc 1\", \"Doc 2\"],            # RAG context\n)\n```\n\n### When to Use Each Field\n\n| Field | Use When |\n|-------|----------|\n| `input` | Always (what you asked) |\n| `actual_output` | Always (what LLM said) |\n| `expected_output` | Testing correctness |\n| `context` | Testing against knowledge base |\n| `retrieval_context` | Testing RAG systems |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Create Your First LLM Test Case\n# =============================================================================\n# A test case captures one LLM interaction for evaluation.\n\nfrom deepeval.test_case import LLMTestCase\n\n# Create a simple test case\ntest_case = LLMTestCase(\n    input=\"How do I authenticate with the Payments API?\",\n    actual_output=\"\"\"To authenticate with the Payments API, use OAuth 2.0:\n    1. Get your client_id and client_secret from the Developer Portal\n    2. Make a POST request to /oauth/token\n    3. Include the access_token in the Authorization header as 'Bearer {token}'\"\"\",\n    expected_output=\"Use OAuth 2.0 with client credentials flow. Get credentials from Developer Portal.\"\n)\n\nprint(\"Test Case Created!\")\nprint(f\"   Input: {test_case.input[:50]}...\")\nprint(f\"   Output: {test_case.actual_output[:50]}...\")\nprint(f\"   Expected: {test_case.expected_output[:50]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## DeepEval Metrics: Measuring Quality\n\nDeepEval provides **50+ metrics** organized by use case:\n\n### Correctness & Accuracy\n| Metric | What It Measures |\n|--------|-----------------|\n| **G-Eval** | Custom criteria (most flexible) |\n| **AnswerRelevancy** | Does output answer the input? |\n| **Correctness** | Factual accuracy vs expected output |\n\n### RAG-Specific\n| Metric | What It Measures |\n|--------|-----------------|\n| **Faithfulness** | Is output grounded in context? |\n| **ContextualRelevancy** | Is retrieved context relevant? |\n| **ContextualPrecision** | How precise is the retrieval? |\n\n### Safety & Quality\n| Metric | What It Measures |\n|--------|-----------------|\n| **Hallucination** | Did LLM make things up? |\n| **Toxicity** | Harmful content detection |\n| **Bias** | Unfair or biased responses |\n\n**For this workshop, we'll focus on G-Eval** - the most flexible and powerful metric.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Test Case Structure\n\n![Test Case Structure](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_02/charts/04_test_case_structure.svg)\n\n**Key Points:**\n- `input` and `actual_output` are always required\n- Other fields depend on which metric you're using\n- G-Eval can use any combination of fields\n- The metric determines which fields matter",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Run Your First Evaluation\n# =============================================================================\n# Let's evaluate a test case with a simple metric.\n\nfrom deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\n# Create test case\ntest_case = LLMTestCase(\n    input=\"What is the capital of France?\",\n    actual_output=\"The capital of France is Paris, a beautiful city known for the Eiffel Tower.\"\n)\n\n# Create metric\nrelevancy_metric = AnswerRelevancyMetric(\n    threshold=0.7,  # Pass if score >= 0.7\n    model=\"gpt-4o-mini\"\n)\n\n# Run evaluation (this runs the batch evaluation)\nprint(\"Running evaluation...\")\nresult = evaluate([test_case], [relevancy_metric])\n\n# Access individual score by measuring directly\nrelevancy_metric.measure(test_case)\n\n# Show results\nprint(\"\\nResults:\")\nprint(f\"   Score: {relevancy_metric.score:.2f}\")\nprint(f\"   Passed: {relevancy_metric.score >= 0.7}\")\nprint(f\"   Reason: {relevancy_metric.reason}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Evaluate Multiple Test Cases\n# =============================================================================\n# In practice, you'll run many test cases at once.\n\nfrom deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\n# Create multiple test cases\ntest_cases = [\n    LLMTestCase(\n        input=\"What is the capital of France?\",\n        actual_output=\"Paris is the capital of France.\"\n    ),\n    LLMTestCase(\n        input=\"What is the capital of France?\",\n        actual_output=\"France is a country in Europe with great food.\"  # Off-topic!\n    ),\n    LLMTestCase(\n        input=\"Who wrote Romeo and Juliet?\",\n        actual_output=\"William Shakespeare wrote Romeo and Juliet in the late 16th century.\"\n    ),\n]\n\n# Create metric\nrelevancy_metric = AnswerRelevancyMetric(\n    threshold=0.7,\n    model=\"gpt-4o-mini\"\n)\n\n# Run evaluation on all test cases\nprint(f\"Evaluating {len(test_cases)} test cases...\")\nresults = evaluate(test_cases, [relevancy_metric])\n\n# Show summary\nprint(\"\\nResults Summary:\")\nfor i, tc in enumerate(test_cases):\n    # Re-run metric on each case for individual scores\n    relevancy_metric.measure(tc)\n    status = \"PASS\" if relevancy_metric.score >= 0.7 else \"FAIL\"\n    print(f\"\\nTest {i+1}: [{status}]\")\n    print(f\"   Input: {tc.input[:40]}...\")\n    print(f\"   Output: {tc.actual_output[:40]}...\")\n    print(f\"   Score: {relevancy_metric.score:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Pytest Integration\n\nDeepEval integrates seamlessly with pytest for CI/CD:\n\n```python\n# test_llm_outputs.py\n\nimport pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\ndef test_capital_question():\n    test_case = LLMTestCase(\n        input=\"What is the capital of France?\",\n        actual_output=\"Paris is the capital of France.\"\n    )\n\n    metric = AnswerRelevancyMetric(threshold=0.7)\n\n    # This raises an exception if the test fails\n    assert_test(test_case, [metric])\n```\n\n**Run with:**\n```bash\ndeepeval test run test_llm_outputs.py\n```\n\n**CI/CD Integration:**\n```yaml\n# .github/workflows/llm-tests.yml\n- name: Run LLM Tests\n  run: deepeval test run tests/\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Key Concept: Thresholds\n\nEvery metric has a **threshold** that determines pass/fail:\n\n```python\nmetric = AnswerRelevancyMetric(\n    threshold=0.7  # Score must be >= 0.7 to pass\n)\n```\n\n### Choosing Thresholds\n\n| Threshold | Use Case |\n|-----------|----------|\n| **0.9+** | Critical accuracy (medical, legal) |\n| **0.7-0.9** | Standard quality (most applications) |\n| **0.5-0.7** | Lenient (creative tasks, drafts) |\n\n### Best Practice: Start Low, Increase Over Time\n\n1. Start with `threshold=0.5` to establish baseline\n2. Measure current performance\n3. Gradually increase as you improve prompts\n4. Different metrics may need different thresholds",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Impact of Different Thresholds\n# =============================================================================\n# Same test case, different thresholds = different results.\n\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\n# A borderline response\ntest_case = LLMTestCase(\n    input=\"What is the capital of France?\",\n    actual_output=\"Paris, known for the Eiffel Tower, is a major European city.\"\n)\n\n# Test with different thresholds\nthresholds = [0.5, 0.7, 0.9]\n\nprint(\"Same response, different thresholds:\")\nprint(\"-\" * 50)\n\nfor threshold in thresholds:\n    metric = AnswerRelevancyMetric(\n        threshold=threshold,\n        model=\"gpt-4o-mini\"\n    )\n    metric.measure(test_case)\n\n    status = \"PASS\" if metric.score >= threshold else \"FAIL\"\n    print(f\"\\nThreshold: {threshold}\")\n    print(f\"   Score: {metric.score:.2f}\")\n    print(f\"   Result: [{status}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Coming Up: Lab 1\n\nIn Lab 1, you'll build an evaluation harness for DevHub that:\n\n1. Creates test cases for different query types:\n   - Documentation search\n   - Owner lookup\n   - Status checks\n\n2. Uses G-Eval with custom criteria:\n   - Correctness\n   - Helpfulness\n   - Handling edge cases\n\n3. Runs automated evaluation:\n   - 10+ test cases\n   - Multiple metrics\n   - Clear pass/fail results\n\nBut first, we need to understand G-Eval - the most powerful metric in DeepEval.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Key Takeaways: DeepEval Basics\n\n1. **Test Cases** capture LLM interactions (input, output, optional fields)\n\n2. **Metrics** define what \"good\" means (50+ available)\n\n3. **Thresholds** determine pass/fail (start low, increase over time)\n\n4. **Pytest integration** enables CI/CD pipelines\n\n5. **Batch evaluation** tests many cases quickly (100 in 3 minutes)\n\n---\n\n**Next:** Deep dive into G-Eval - the most flexible and powerful metric.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Topic 3: G-Eval - LLM-as-Judge with Chain-of-Thought\n\nG-Eval is the **most flexible metric** in DeepEval. It lets you define **any evaluation criteria** in plain English, then uses an LLM to score against that criteria.\n\nThink of it as having an expert reviewer who follows your exact rubric.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## How G-Eval Works\n\n### The G-Eval Algorithm (from the research paper)\n\n1. **Criteria Definition**: You define what \"good\" looks like in plain English\n\n2. **CoT Generation**: G-Eval generates evaluation steps (chain of thought)\n\n3. **Form-Filling**: Creates a structured evaluation prompt\n\n4. **Token Probabilities**: Uses LLM output probabilities for scoring\n\n5. **Weighted Summation**: Normalizes scores (1-5 scale internally, 0-1 output)\n\n### Why This Matters\n\n- **0.87 correlation** with human judgment (better than human-human!)\n- **Consistent** across evaluations (unlike raw LLM scoring)\n- **Customizable** for any use case\n- **Explainable** - provides reasoning for each score",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## G-Eval Evaluation Pipeline\n\n![G-Eval Pipeline](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_02/charts/03_geval_pipeline.svg)\n\n**Step-by-step:**\n\n1. **Your Criteria** → \"Is the response helpful and actionable?\"\n\n2. **CoT Generation** → LLM generates evaluation steps:\n   - Check if response addresses the question\n   - Check if it provides specific actions\n   - Check if information is accurate\n\n3. **Form-Filling** → Structured prompt with test case data\n\n4. **LLM Scoring** → Model outputs score with probabilities\n\n5. **Normalization** → Token probabilities weighted for final score",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## G-Eval Configuration\n\n### Required Parameters\n\n```python\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCaseParams\n\nmetric = GEval(\n    name=\"Helpfulness\",                    # Metric name (for reports)\n    criteria=\"Is the response helpful?\",   # What to evaluate\n    evaluation_params=[                    # Which test case fields to use\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT\n    ],\n)\n```\n\n### Optional Parameters\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `threshold` | 0.5 | Pass/fail cutoff |\n| `model` | gpt-4o | LLM for judging |\n| `evaluation_steps` | Auto-generated | Custom evaluation steps |\n| `strict_mode` | False | Only pass if score == 1.0 |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Basic G-Eval Usage\n# =============================================================================\n# G-Eval with a simple criteria.\n\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\n# Define the metric\nhelpfulness_metric = GEval(\n    name=\"Helpfulness\",\n    criteria=\"Determine if the response provides helpful, actionable information that directly addresses the user's question.\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT\n    ],\n    threshold=0.7,\n    model=\"gpt-4o-mini\"\n)\n\n# Create test case\ntest_case = LLMTestCase(\n    input=\"How do I reset my password?\",\n    actual_output=\"\"\"To reset your password:\n    1. Go to the login page\n    2. Click 'Forgot Password'\n    3. Enter your email\n    4. Check your inbox for the reset link\n    5. Create a new password (8+ characters, include numbers)\"\"\"\n)\n\n# Evaluate\nhelpfulness_metric.measure(test_case)\n\nprint(\"G-Eval Helpfulness Results:\")\nprint(f\"   Score: {helpfulness_metric.score:.2f}\")\nprint(f\"   Passed: {helpfulness_metric.score >= 0.7}\")\nprint(f\"   Reason: {helpfulness_metric.reason}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Multiple G-Eval Metrics\n# =============================================================================\n# Evaluate the same response against multiple criteria.\n\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\n# Define multiple metrics\nmetrics = [\n    GEval(\n        name=\"Correctness\",\n        criteria=\"Is the response factually correct and accurate?\",\n        evaluation_params=[\n            LLMTestCaseParams.INPUT,\n            LLMTestCaseParams.ACTUAL_OUTPUT,\n            LLMTestCaseParams.EXPECTED_OUTPUT\n        ],\n        threshold=0.8,\n        model=\"gpt-4o-mini\"\n    ),\n    GEval(\n        name=\"Coherence\",\n        criteria=\"Is the response logically structured and easy to follow?\",\n        evaluation_params=[\n            LLMTestCaseParams.ACTUAL_OUTPUT\n        ],\n        threshold=0.7,\n        model=\"gpt-4o-mini\"\n    ),\n    GEval(\n        name=\"Conciseness\",\n        criteria=\"Is the response appropriately concise without unnecessary information?\",\n        evaluation_params=[\n            LLMTestCaseParams.INPUT,\n            LLMTestCaseParams.ACTUAL_OUTPUT\n        ],\n        threshold=0.6,\n        model=\"gpt-4o-mini\"\n    ),\n]\n\n# Create test case\ntest_case = LLMTestCase(\n    input=\"What is the capital of France?\",\n    actual_output=\"\"\"The capital of France is Paris. Paris is located in northern France\n    along the Seine River. It's known as the City of Light and is famous for the Eiffel Tower,\n    the Louvre Museum, and Notre-Dame Cathedral. The city has a population of about 2 million\n    in the city proper and over 12 million in the metropolitan area.\"\"\",\n    expected_output=\"Paris\"\n)\n\n# Evaluate with all metrics\nprint(\"Multi-Metric Evaluation:\")\nprint(\"=\" * 60)\n\nfor metric in metrics:\n    metric.measure(test_case)\n    status = \"PASS\" if metric.score >= metric.threshold else \"FAIL\"\n    print(f\"\\n{metric.name}:\")\n    print(f\"   Score: {metric.score:.2f} (threshold: {metric.threshold})\")\n    print(f\"   Result: [{status}]\")\n    print(f\"   Reason: {metric.reason[:100]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Choosing Evaluation Parameters\n\nThe `evaluation_params` tell G-Eval which test case fields to consider:\n\n### Available Parameters\n\n```python\nfrom deepeval.test_case import LLMTestCaseParams\n\nLLMTestCaseParams.INPUT           # The user's question/prompt\nLLMTestCaseParams.ACTUAL_OUTPUT   # The LLM's response\nLLMTestCaseParams.EXPECTED_OUTPUT # Ground truth (if available)\nLLMTestCaseParams.CONTEXT         # Knowledge base content\nLLMTestCaseParams.RETRIEVAL_CONTEXT  # RAG retrieved documents\n```\n\n### Common Combinations\n\n| Use Case | Parameters |\n|----------|------------|\n| **Answer relevancy** | INPUT + ACTUAL_OUTPUT |\n| **Correctness** | INPUT + ACTUAL_OUTPUT + EXPECTED_OUTPUT |\n| **Faithfulness (RAG)** | ACTUAL_OUTPUT + CONTEXT |\n| **Groundedness** | ACTUAL_OUTPUT + RETRIEVAL_CONTEXT |\n| **Coherence** | ACTUAL_OUTPUT only |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: G-Eval with Custom Evaluation Steps\n# =============================================================================\n# You can provide your own evaluation steps instead of auto-generating.\n\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\n# Define metric with explicit evaluation steps\napi_quality_metric = GEval(\n    name=\"API Documentation Quality\",\n    criteria=\"Does the response provide high-quality API documentation guidance?\",\n    evaluation_steps=[\n        \"Check if the response includes the correct API endpoint or method\",\n        \"Verify that authentication requirements are mentioned\",\n        \"Confirm that example code or usage patterns are provided\",\n        \"Assess if error handling guidance is included\",\n        \"Evaluate if the response is actionable for a developer\"\n    ],\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT\n    ],\n    threshold=0.7,\n    model=\"gpt-4o-mini\"\n)\n\n# Test case\ntest_case = LLMTestCase(\n    input=\"How do I authenticate with the Payments API?\",\n    actual_output=\"\"\"To authenticate with the Payments API:\n\n1. **Get Credentials**: Obtain your client_id and client_secret from the Developer Portal.\n\n2. **Request Token**:\n```\nPOST /oauth/token\nContent-Type: application/x-www-form-urlencoded\n\ngrant_type=client_credentials&client_id=YOUR_ID&client_secret=YOUR_SECRET\n```\n\n3. **Use Token**: Include in all requests:\n```\nAuthorization: Bearer YOUR_ACCESS_TOKEN\n```\n\n**Note**: Tokens expire after 1 hour. Handle 401 errors by refreshing the token.\"\"\"\n)\n\n# Evaluate\napi_quality_metric.measure(test_case)\n\nprint(\"API Documentation Quality Evaluation:\")\nprint(f\"   Score: {api_quality_metric.score:.2f}\")\nprint(f\"   Reason: {api_quality_metric.reason}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Designing Metrics for DevHub\n\nFor our DevHub application (from Session 1), we need metrics that capture its specific quality needs:\n\n### Metric 1: Documentation Helpfulness\n**For:** `search_docs` tool responses\n**Criteria:** Does the response provide helpful documentation that addresses the query?\n\n### Metric 2: Owner Information Accuracy\n**For:** `find_owner` tool responses\n**Criteria:** Is the owner information accurate and up-to-date?\n\n### Metric 3: Status Clarity\n**For:** `check_status` tool responses\n**Criteria:** Is the status clearly communicated with actionable information?\n\n### Metric 4: Stale Data Handling\n**For:** Any response with inactive owners\n**Criteria:** Does the response appropriately flag stale or inactive information?\n\n### Metric 5: Error Acknowledgment\n**For:** Responses when tools fail\n**Criteria:** Does the response acknowledge errors honestly without making things up?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: DevHub-Specific G-Eval Metrics\n# =============================================================================\n# Metrics tailored for our DevHub use cases.\n\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCaseParams\n\n# Metric 1: Documentation Helpfulness\ndocs_helpfulness = GEval(\n    name=\"Documentation Helpfulness\",\n    criteria=\"\"\"Evaluate if the response provides helpful documentation guidance that:\n    1. Directly addresses the user's question\n    2. Includes specific, actionable steps\n    3. References relevant code examples or endpoints when appropriate\n    4. Is technically accurate\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT\n    ],\n    threshold=0.7,\n    model=\"gpt-4o-mini\"\n)\n\n# Metric 2: Stale Data Handling\nstale_data_handling = GEval(\n    name=\"Stale Data Handling\",\n    criteria=\"\"\"Evaluate if the response appropriately handles potentially stale data:\n    1. If an owner is marked as inactive, this should be mentioned\n    2. Alternative contacts (team channel) should be suggested\n    3. The response should not present stale info as current\n    4. Any uncertainty about data freshness should be acknowledged\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT\n    ],\n    threshold=0.8,  # Higher threshold for data quality\n    model=\"gpt-4o-mini\"\n)\n\nprint(\"DevHub Metrics Defined!\")\nprint(f\"   1. {docs_helpfulness.name} (threshold: {docs_helpfulness.threshold})\")\nprint(f\"   2. {stale_data_handling.name} (threshold: {stale_data_handling.threshold})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## G-Eval Best Practices\n\n### 1. Write Clear Criteria\n```python\n# BAD - vague\ncriteria=\"Is the response good?\"\n\n# GOOD - specific\ncriteria=\"Does the response provide accurate API authentication steps including endpoint, required parameters, and example code?\"\n```\n\n### 2. Choose Appropriate Thresholds\n- Start with **0.5-0.6** for new metrics\n- Measure baseline performance\n- Increase threshold as prompts improve\n- Critical metrics: **0.8-0.9**\n\n### 3. Use Custom Evaluation Steps for Complex Criteria\n- Auto-generated steps work for simple criteria\n- Complex domains benefit from explicit steps\n- Steps should be checkable (yes/no or scale)\n\n### 4. Test Your Metrics\n- Run on known good/bad examples\n- Verify scores match expectations\n- Adjust criteria if needed\n\n### 5. Name Metrics Descriptively\n- Good: `\"API_Auth_Completeness\"`\n- Bad: `\"Metric1\"`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DEMO: Testing G-Eval Metrics\n# =============================================================================\n# Always test your metrics on known good/bad examples.\n\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\n# Create a metric\nclarity_metric = GEval(\n    name=\"Response Clarity\",\n    criteria=\"Is the response clear, well-structured, and easy to understand?\",\n    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n    threshold=0.7,\n    model=\"gpt-4o-mini\"\n)\n\n# Test on known examples\ntest_examples = [\n    {\n        \"name\": \"Good response (should score high)\",\n        \"output\": \"\"\"Here are the steps to reset your password:\n        1. Go to login page\n        2. Click \"Forgot Password\"\n        3. Enter your email\n        4. Check inbox for reset link\n        Done!\"\"\"\n    },\n    {\n        \"name\": \"Bad response (should score low)\",\n        \"output\": \"password reset you need to do stuff with email thing and then click and login idk it's confusing just ask someone\"\n    }\n]\n\nprint(\"Testing Clarity Metric:\")\nprint(\"=\" * 60)\n\nfor example in test_examples:\n    test_case = LLMTestCase(\n        input=\"How do I reset my password?\",\n        actual_output=example[\"output\"]\n    )\n    clarity_metric.measure(test_case)\n\n    print(f\"\\n{example['name']}\")\n    print(f\"   Score: {clarity_metric.score:.2f}\")\n    expected = \"High (>0.7)\" if \"Good\" in example[\"name\"] else \"Low (<0.7)\"\n    print(f\"   Expected: {expected}\")\n    matches = (clarity_metric.score > 0.7) == (\"Good\" in example[\"name\"])\n    print(f\"   Matches: {'Yes' if matches else 'NO - Adjust metric!'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## When to Use G-Eval vs Built-in Metrics\n\n### Use G-Eval When:\n- You need **custom criteria** specific to your domain\n- Built-in metrics don't capture what you need\n- You want **explainable scoring** with reasons\n- Evaluating **tone, style, or domain-specific requirements**\n\n### Use Built-in Metrics When:\n- Standard evaluations (relevancy, faithfulness)\n- **RAG systems** (ContextualPrecision, Faithfulness)\n- **Safety checks** (Toxicity, Bias)\n- You want **faster evaluation** (some built-ins are optimized)\n\n### Common Combinations\n\n```python\n# For a RAG chatbot:\nmetrics = [\n    AnswerRelevancyMetric(),           # Is it relevant?\n    FaithfulnessMetric(),              # Is it grounded in context?\n    GEval(criteria=\"Is it helpful?\")   # Custom quality check\n]\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Key Takeaways: G-Eval\n\n1. **G-Eval is the most flexible metric** - define any criteria in plain English\n\n2. **Chain-of-thought reasoning** provides explainable scores\n\n3. **0.87 correlation** with human judgment - better than human-human!\n\n4. **Choose parameters carefully** - they determine what's evaluated\n\n5. **Test your metrics** on known good/bad examples before deploying\n\n---\n\n**Next:** Lab 1 - Build an evaluation harness for DevHub using G-Eval.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Lab 1: Build an Evaluation Harness for DevHub\n\nTime to get your hands dirty! In this lab, you'll build a complete testing system for the instrumented DevHub from Session 1.\n\n**Duration:** ~30 minutes\n\n**What you'll do:**\n1. Load DevHub (instrumented version from Session 1)\n2. Create test cases for different query types\n3. Define G-Eval metrics for quality dimensions\n4. Run automated evaluation\n5. Analyze results\n\n**Scaffolding decreases** as you progress:\n- Task 1: Full guidance\n- Task 2: Medium guidance\n- Task 3: Minimal guidance",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## What You'll Build\n\nBy the end of this lab, you'll have:\n\n```\nDevHub Evaluation Harness\n├── Test Cases (10+)\n│   ├── Documentation queries\n│   ├── Owner lookup queries\n│   └── Status check queries\n│\n├── G-Eval Metrics (3+)\n│   ├── Response Correctness\n│   ├── Response Helpfulness\n│   └── Edge Case Handling\n│\n└── Evaluation Results\n    ├── Per-test scores\n    ├── Aggregate metrics\n    └── Pass/fail summary\n```\n\nThis harness can be run automatically to catch regressions!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# LOAD DEVHUB (INSTRUMENTED VERSION)\n# =============================================================================\n# We'll use the instrumented DevHub from Session 1.\n# This version has OpenTelemetry tracing built in!\n\nimport json\nfrom openai import OpenAI\n\n# Simplified DevHub for testing (mirrors the instrumented version structure)\nclass DevHubSimple:\n    \"\"\"Simplified DevHub for evaluation testing.\n\n    This mirrors the structure of the instrumented DevHub from Session 1,\n    with the same data sources and query patterns.\n    \"\"\"\n\n    def __init__(self):\n        self.client = OpenAI()\n\n        # Same data as instrumented DevHub\n        self.docs = {\n            \"payments-auth\": \"Use OAuth 2.0 with client_id and client_secret from Developer Portal.\",\n            \"error-handling\": \"Return standard error format with code, message, and correlation_id.\",\n            \"rate-limiting\": \"100 requests/minute authenticated, 10 requests/minute unauthenticated.\",\n            \"search-api\": \"POST /api/v1/search with query in body. Returns ranked results.\",\n        }\n        self.owners = {\n            \"billing\": {\"name\": \"Sarah Chen\", \"slack\": \"@sarah.chen\", \"team\": \"payments-team\", \"is_active\": True},\n            \"vector-search\": {\"name\": \"David Kim\", \"slack\": \"@david.kim\", \"team\": \"search-team\", \"is_active\": False},  # STALE!\n            \"auth\": {\"name\": \"Michael Brown\", \"slack\": \"@mbrown\", \"team\": \"security-team\", \"is_active\": True},\n            \"api-gateway\": {\"name\": \"Lisa Wang\", \"slack\": \"@lwang\", \"team\": \"platform-team\", \"is_active\": True},\n        }\n        self.status = {\n            \"payments-api\": {\"status\": \"healthy\", \"uptime\": 99.95},\n            \"staging\": {\"status\": \"degraded\", \"incident\": \"Database connection pool exhaustion\"},\n            \"search-api\": {\"status\": \"healthy\", \"uptime\": 99.99},\n            \"auth-service\": {\"status\": \"healthy\", \"uptime\": 99.90},\n        }\n\n    def query(self, user_query: str) -> str:\n        \"\"\"Process a query and return response.\"\"\"\n        context = f\"\"\"You have access to:\nDocs: {json.dumps(self.docs, indent=2)}\nOwners: {json.dumps(self.owners, indent=2)}\nStatus: {json.dumps(self.status, indent=2)}\"\"\"\n\n        response = self.client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": f\"You are DevHub, an internal developer assistant. Use this data to answer queries accurately. If an owner is_active=False, mention this and suggest using the team channel instead.\\n\\n{context}\"},\n                {\"role\": \"user\", \"content\": user_query}\n            ],\n            temperature=0.3,\n            max_tokens=500\n        )\n        return response.choices[0].message.content\n\n# Initialize\ndevhub = DevHubSimple()\n\nprint(\"DevHub loaded successfully!\")\nprint(\"   Ready for evaluation testing\")\nprint(f\"   Docs: {len(devhub.docs)} entries\")\nprint(f\"   Owners: {len(devhub.owners)} entries\")\nprint(f\"   Services: {len(devhub.status)} entries\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Task 1: Create Test Cases\n\n**Goal:** Create test cases that cover DevHub's main functionality.\n\n**What to do:**\n\n1. Create 3+ test cases for **documentation queries**\n2. Create 3+ test cases for **owner lookup queries**\n3. Create 3+ test cases for **status check queries**\n\nFor each test case, include:\n- `input`: The user's question\n- `actual_output`: DevHub's response (from `devhub.query()`)\n- `expected_output`: What a good answer should contain\n\n**Example:**\n```python\ntest_case = LLMTestCase(\n    input=\"How do I authenticate with the Payments API?\",\n    actual_output=devhub.query(\"How do I authenticate with the Payments API?\"),\n    expected_output=\"Use OAuth 2.0 with client credentials\"\n)\n```\n\n**Time:** ~10 minutes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TASK 1: Create Test Cases (SOLUTION)\n# =============================================================================\n# Create test cases for DevHub evaluation.\n# =============================================================================\n\nfrom deepeval.test_case import LLMTestCase\n\n# -----------------------------------------------------------------------------\n# Documentation Test Cases\n# -----------------------------------------------------------------------------\ndoc_test_cases = [\n    LLMTestCase(\n        input=\"How do I authenticate with the Payments API?\",\n        actual_output=devhub.query(\"How do I authenticate with the Payments API?\"),\n        expected_output=\"Use OAuth 2.0 with client_id and client_secret from Developer Portal\"\n    ),\n    LLMTestCase(\n        input=\"What is the error handling format?\",\n        actual_output=devhub.query(\"What is the error handling format?\"),\n        expected_output=\"Standard error format with code, message, and correlation_id\"\n    ),\n    LLMTestCase(\n        input=\"What are the rate limits?\",\n        actual_output=devhub.query(\"What are the rate limits?\"),\n        expected_output=\"100 requests/minute authenticated, 10/minute unauthenticated\"\n    ),\n]\n\n# -----------------------------------------------------------------------------\n# Owner Lookup Test Cases\n# -----------------------------------------------------------------------------\nowner_test_cases = [\n    LLMTestCase(\n        input=\"Who owns billing?\",\n        actual_output=devhub.query(\"Who owns billing?\"),\n        expected_output=\"Sarah Chen (@sarah.chen) from payments-team - active owner\"\n    ),\n    LLMTestCase(\n        input=\"Who owns vector-search?\",\n        actual_output=devhub.query(\"Who owns vector-search?\"),\n        expected_output=\"David Kim is listed but is INACTIVE - contact #search-team channel instead\"\n    ),\n    LLMTestCase(\n        input=\"Who owns auth?\",\n        actual_output=devhub.query(\"Who owns auth?\"),\n        expected_output=\"Michael Brown (@mbrown) from security-team - active owner\"\n    ),\n]\n\n# -----------------------------------------------------------------------------\n# Status Check Test Cases\n# -----------------------------------------------------------------------------\nstatus_test_cases = [\n    LLMTestCase(\n        input=\"Is the payments API working?\",\n        actual_output=devhub.query(\"Is the payments API working?\"),\n        expected_output=\"Payments API is healthy with 99.95% uptime\"\n    ),\n    LLMTestCase(\n        input=\"Is staging working?\",\n        actual_output=devhub.query(\"Is staging working?\"),\n        expected_output=\"Staging is DEGRADED due to database connection pool exhaustion\"\n    ),\n    LLMTestCase(\n        input=\"What is the status of all services?\",\n        actual_output=devhub.query(\"What is the status of all services?\"),\n        expected_output=\"List all services: payments-api healthy, staging degraded, search-api healthy, auth-service healthy\"\n    ),\n]\n\n# -----------------------------------------------------------------------------\n# Combine all test cases\n# -----------------------------------------------------------------------------\nall_test_cases = doc_test_cases + owner_test_cases + status_test_cases\n\nprint(f\"Total test cases created: {len(all_test_cases)}\")\nprint(f\"   Documentation: {len(doc_test_cases)}\")\nprint(f\"   Owner lookup: {len(owner_test_cases)}\")\nprint(f\"   Status check: {len(status_test_cases)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# SOLUTION: Task 1 - Create Test Cases\n# =============================================================================\n# Expand if you get stuck.\n\nfrom deepeval.test_case import LLMTestCase\n\n# Documentation Test Cases\ndoc_test_cases = [\n    LLMTestCase(\n        input=\"How do I authenticate with the Payments API?\",\n        actual_output=devhub.query(\"How do I authenticate with the Payments API?\"),\n        expected_output=\"Use OAuth 2.0 with client_id and client_secret from Developer Portal\"\n    ),\n    LLMTestCase(\n        input=\"What is the error handling format?\",\n        actual_output=devhub.query(\"What is the error handling format?\"),\n        expected_output=\"Standard error format with code, message, and correlation_id\"\n    ),\n    LLMTestCase(\n        input=\"What are the rate limits?\",\n        actual_output=devhub.query(\"What are the rate limits?\"),\n        expected_output=\"100 requests/minute authenticated, 10/minute unauthenticated\"\n    ),\n    LLMTestCase(\n        input=\"How do I use the search API?\",\n        actual_output=devhub.query(\"How do I use the search API?\"),\n        expected_output=\"POST /api/v1/search with query in body, returns ranked results\"\n    ),\n]\n\n# Owner Lookup Test Cases\nowner_test_cases = [\n    LLMTestCase(\n        input=\"Who owns billing?\",\n        actual_output=devhub.query(\"Who owns billing?\"),\n        expected_output=\"Sarah Chen (@sarah.chen) from payments-team - active owner\"\n    ),\n    LLMTestCase(\n        input=\"Who owns vector-search?\",\n        actual_output=devhub.query(\"Who owns vector-search?\"),\n        expected_output=\"David Kim is listed but is INACTIVE - contact #search-team channel instead\"\n    ),\n    LLMTestCase(\n        input=\"Who owns auth?\",\n        actual_output=devhub.query(\"Who owns auth?\"),\n        expected_output=\"Michael Brown (@mbrown) from security-team - active owner\"\n    ),\n]\n\n# Status Check Test Cases\nstatus_test_cases = [\n    LLMTestCase(\n        input=\"Is the payments API working?\",\n        actual_output=devhub.query(\"Is the payments API working?\"),\n        expected_output=\"Payments API is healthy with 99.95% uptime\"\n    ),\n    LLMTestCase(\n        input=\"Is staging working?\",\n        actual_output=devhub.query(\"Is staging working?\"),\n        expected_output=\"Staging is DEGRADED due to database connection pool exhaustion\"\n    ),\n    LLMTestCase(\n        input=\"What is the status of all services?\",\n        actual_output=devhub.query(\"What is the status of all services?\"),\n        expected_output=\"List all services: payments-api healthy, staging degraded, search-api healthy, auth-service healthy\"\n    ),\n]\n\nall_test_cases = doc_test_cases + owner_test_cases + status_test_cases\nprint(f\"Solution: {len(all_test_cases)} test cases created\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Task 2: Define G-Eval Metrics\n\n**Goal:** Create G-Eval metrics to evaluate DevHub responses.\n\n**What to do:**\n\nCreate at least 3 metrics:\n\n1. **Correctness**: Is the information accurate?\n2. **Helpfulness**: Is the response actionable and useful?\n3. **Edge Case Handling**: Does it handle stale data/degraded services properly?\n\n**Hints:**\n- Use appropriate `evaluation_params` for each metric\n- Set thresholds based on importance (critical = 0.8+)\n- Write clear, specific criteria\n\n**Time:** ~8 minutes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TASK 2: Define G-Eval Metrics (SOLUTION)\n# =============================================================================\n# Create metrics to evaluate DevHub response quality.\n# =============================================================================\n\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCaseParams\n\n# -----------------------------------------------------------------------------\n# Metric 1: Correctness\n# -----------------------------------------------------------------------------\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=\"\"\"Evaluate if the response provides factually correct information that aligns with the expected output.\n    Consider:\n    1. Are the key facts accurate?\n    2. Does it match the expected answer?\n    3. Are there any factual errors?\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT\n    ],\n    threshold=0.8,\n    model=\"gpt-4o-mini\"\n)\n\n# -----------------------------------------------------------------------------\n# Metric 2: Helpfulness\n# -----------------------------------------------------------------------------\nhelpfulness_metric = GEval(\n    name=\"Helpfulness\",\n    criteria=\"\"\"Evaluate if the response is helpful and actionable:\n    1. Does it directly address the user's question?\n    2. Does it provide specific, actionable information?\n    3. Would a developer be able to proceed based on this response?\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT\n    ],\n    threshold=0.7,\n    model=\"gpt-4o-mini\"\n)\n\n# -----------------------------------------------------------------------------\n# Metric 3: Edge Case Handling\n# -----------------------------------------------------------------------------\nedge_case_metric = GEval(\n    name=\"Edge Case Handling\",\n    criteria=\"\"\"Evaluate if the response appropriately handles edge cases:\n    1. If an owner is inactive, is this clearly communicated?\n    2. If a service is degraded, is the issue explained?\n    3. Does it suggest alternatives when appropriate?\n    4. Is uncertainty acknowledged rather than hidden?\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT\n    ],\n    threshold=0.75,\n    model=\"gpt-4o-mini\"\n)\n\n# -----------------------------------------------------------------------------\n# Collect metrics\n# -----------------------------------------------------------------------------\nall_metrics = [correctness_metric, helpfulness_metric, edge_case_metric]\n\nprint(f\"Metrics defined: {len(all_metrics)}\")\nfor m in all_metrics:\n    print(f\"   - {m.name} (threshold: {m.threshold})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# SOLUTION: Task 2 - Define G-Eval Metrics\n# =============================================================================\n\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCaseParams\n\n# Metric 1: Correctness\ncorrectness_metric = GEval(\n    name=\"Correctness\",\n    criteria=\"\"\"Evaluate if the response provides factually correct information that aligns with the expected output.\n    Consider:\n    1. Are the key facts accurate?\n    2. Does it match the expected answer?\n    3. Are there any factual errors?\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT\n    ],\n    threshold=0.8,\n    model=\"gpt-4o-mini\"\n)\n\n# Metric 2: Helpfulness\nhelpfulness_metric = GEval(\n    name=\"Helpfulness\",\n    criteria=\"\"\"Evaluate if the response is helpful and actionable:\n    1. Does it directly address the user's question?\n    2. Does it provide specific, actionable information?\n    3. Would a developer be able to proceed based on this response?\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT\n    ],\n    threshold=0.7,\n    model=\"gpt-4o-mini\"\n)\n\n# Metric 3: Edge Case Handling\nedge_case_metric = GEval(\n    name=\"Edge Case Handling\",\n    criteria=\"\"\"Evaluate if the response appropriately handles edge cases:\n    1. If an owner is inactive, is this clearly communicated?\n    2. If a service is degraded, is the issue explained?\n    3. Does it suggest alternatives when appropriate?\n    4. Is uncertainty acknowledged rather than hidden?\"\"\",\n    evaluation_params=[\n        LLMTestCaseParams.INPUT,\n        LLMTestCaseParams.ACTUAL_OUTPUT,\n        LLMTestCaseParams.EXPECTED_OUTPUT\n    ],\n    threshold=0.75,\n    model=\"gpt-4o-mini\"\n)\n\nall_metrics = [correctness_metric, helpfulness_metric, edge_case_metric]\nprint(f\"Solution: {len(all_metrics)} metrics defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Task 3: Run Evaluation\n\n**Goal:** Run all test cases against all metrics and analyze results.\n\n**What to do:**\n\n1. Use `deepeval.evaluate()` to run all tests\n2. Print per-test results with pass/fail\n3. Calculate aggregate statistics\n\n**No starter code** - apply what you've learned!\n\n**Time:** ~10 minutes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TASK 3: Run Evaluation (SOLUTION)\n# =============================================================================\n# Run all test cases against all metrics.\n# =============================================================================\n\nfrom deepeval import evaluate\nimport time\n\nprint(\"Running DevHub Evaluation...\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\n# 1. Run evaluation\nresults = evaluate(all_test_cases, all_metrics)\n\n# 2. Print per-test results\nprint(\"\\nPer-Test Results:\")\nprint(\"-\" * 60)\n\npassed = 0\nfailed = 0\n\nfor i, test_case in enumerate(all_test_cases):\n    test_passed = True\n    print(f\"\\nTest {i+1}: {test_case.input[:40]}...\")\n\n    for metric in all_metrics:\n        metric.measure(test_case)\n        status = \"PASS\" if metric.score >= metric.threshold else \"FAIL\"\n        if metric.score < metric.threshold:\n            test_passed = False\n        print(f\"   {metric.name}: {metric.score:.2f} [{status}]\")\n\n    if test_passed:\n        passed += 1\n    else:\n        failed += 1\n\n# 3. Calculate and print summary\nelapsed = time.time() - start_time\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"Total Tests: {len(all_test_cases)}\")\nprint(f\"Passed: {passed}\")\nprint(f\"Failed: {failed}\")\nprint(f\"Pass Rate: {passed/len(all_test_cases)*100:.1f}%\")\nprint(f\"Time: {elapsed:.1f} seconds\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# SOLUTION: Task 3 - Run Evaluation\n# =============================================================================\n\nfrom deepeval import evaluate\nimport time\n\nprint(\"Running DevHub Evaluation...\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\n# Run evaluation\nresults = evaluate(all_test_cases, all_metrics)\n\n# Per-test results\nprint(\"\\nPer-Test Results:\")\nprint(\"-\" * 60)\n\npassed = 0\nfailed = 0\n\nfor i, test_case in enumerate(all_test_cases):\n    test_passed = True\n    print(f\"\\nTest {i+1}: {test_case.input[:40]}...\")\n\n    for metric in all_metrics:\n        metric.measure(test_case)\n        status = \"PASS\" if metric.score >= metric.threshold else \"FAIL\"\n        if metric.score < metric.threshold:\n            test_passed = False\n        print(f\"   {metric.name}: {metric.score:.2f} [{status}]\")\n\n    if test_passed:\n        passed += 1\n    else:\n        failed += 1\n\n# Summary\nelapsed = time.time() - start_time\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"Total Tests: {len(all_test_cases)}\")\nprint(f\"Passed: {passed}\")\nprint(f\"Failed: {failed}\")\nprint(f\"Pass Rate: {passed/len(all_test_cases)*100:.1f}%\")\nprint(f\"Time: {elapsed:.1f} seconds\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Lab 1 Verification Checklist\n\nVerify your evaluation harness:\n\n### 1. Test Cases\n- [ ] At least 10 test cases created\n- [ ] Mix of documentation, owner, and status queries\n- [ ] Each has input, actual_output, expected_output\n\n### 2. Metrics\n- [ ] At least 3 G-Eval metrics defined\n- [ ] Each has clear criteria\n- [ ] Thresholds set appropriately\n\n### 3. Results\n- [ ] All tests ran without errors\n- [ ] Got scores for each test/metric combination\n- [ ] Summary shows pass/fail counts\n\n### 4. Analysis\n- [ ] Can identify which tests failed\n- [ ] Can see which metrics need improvement\n- [ ] Understand why failures occurred\n\n**If all checked, you've built an evaluation harness!**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Lab 1 Key Takeaways\n\n1. **Test cases capture LLM interactions** for systematic evaluation\n\n2. **G-Eval metrics define quality** in plain English\n\n3. **Batch evaluation** runs many tests quickly (~3 min for 100 tests)\n\n4. **Results show exactly where** quality issues occur\n\n5. **This harness can run on every prompt change** to catch regressions!\n\n---\n\n**Next:** Lab 2 - Generate synthetic test data and run regression tests.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Lab 2: Synthetic Data Generation & Regression Testing\n\nNow that you can evaluate DevHub, let's scale up your test suite automatically!\n\n**Duration:** ~25 minutes\n\n**What you'll do:**\n1. Generate synthetic test cases from seed examples\n2. Create paraphrased variations of queries\n3. Build a regression testing pipeline\n4. Compare prompt versions\n\n**Why this matters:**\n- 10 seed examples → 100 test cases automatically\n- Catch edge cases you wouldn't think of\n- Ensure prompt changes don't break existing functionality",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Synthetic Data Generation for LLM Testing\n\n### The Problem\nWriting test cases manually is:\n- **Time-consuming**: 10 minutes per good test case\n- **Limited**: You only test what you think of\n- **Biased**: Your examples reflect your assumptions\n\n### The Solution: LLM-Powered Data Generation\n\nUse LLMs to generate test variations:\n1. **Paraphrasing**: Same intent, different wording\n2. **Complexity scaling**: Simple → complex versions\n3. **Adversarial cases**: Edge cases and tricky inputs\n4. **Style variations**: Formal, casual, technical\n\n### DeepEval's Synthesizer\n\n```python\nfrom deepeval.synthesizer import Synthesizer\n\nsynthesizer = Synthesizer()\nsynthetic_data = synthesizer.generate_goldens_from_docs(\n    documents=[\"your documentation...\"],\n    max_goldens_per_document=10\n)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## How Synthetic Data Evolution Works\n\n![Synthetic Data Evolution](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_02/charts/05_synthetic_data_evolution.svg)\n\n**Process:**\n\n1. **Seed Examples** (10 hand-crafted)\n   - High-quality, verified test cases\n   - Cover main functionality\n\n2. **Filtration**\n   - Remove low-quality seeds\n   - Ensure diversity\n\n3. **Evolution**\n   - Generate paraphrases\n   - Increase complexity\n   - Add adversarial variations\n\n4. **Styling**\n   - Apply tone variations\n   - Formal/casual/technical\n\n5. **Golden Dataset** (100+ test cases)\n   - Stratified by type and difficulty\n   - Ready for regression testing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# SYNTHETIC DATA: DeepEval Synthesizer Setup\n# =============================================================================\n# Using DeepEval's built-in Synthesizer with document files.\n# This provides research-backed generation with quality filtering.\n\nfrom deepeval.synthesizer import Synthesizer\nimport tempfile\nimport os\n\n# Initialize synthesizer\nsynthesizer = Synthesizer(model=\"gpt-4o-mini\")\n\n# Create temporary document files from DevHub's data\n# (In production, you'd use your actual documentation files)\ntemp_dir = tempfile.mkdtemp()\ndocument_paths = []\n\n# Write DevHub data to temp files\ndocs_content = {\n    \"payments_auth.txt\": f\"Payments API Authentication: {devhub.docs['payments-auth']}\",\n    \"error_handling.txt\": f\"Error Handling Standards: {devhub.docs['error-handling']}\",\n    \"rate_limiting.txt\": f\"Rate Limiting Policy: {devhub.docs['rate-limiting']}\",\n    \"ownership.txt\": \"\"\"Service Ownership Information:\n- Billing: Sarah Chen (@sarah.chen) from payments-team, currently active.\n- Vector Search: David Kim (@david.kim) from search-team, currently INACTIVE - use team channel instead.\n- Auth: Michael Brown (@mbrown) from security-team, currently active.\"\"\",\n    \"status.txt\": \"\"\"Service Status Information:\n- Payments API: Currently healthy with 99.95% uptime.\n- Staging Environment: Currently DEGRADED due to database connection pool exhaustion.\n- Search API: Currently healthy with 99.99% uptime.\"\"\",\n}\n\nfor filename, content in docs_content.items():\n    path = os.path.join(temp_dir, filename)\n    with open(path, 'w') as f:\n        f.write(content)\n    document_paths.append(path)\n\nprint(\"Synthesizer initialized!\")\nprint(f\"   Model: gpt-4o-mini\")\nprint(f\"   Document files created: {len(document_paths)}\")\nfor p in document_paths:\n    print(f\"      - {os.path.basename(p)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# TASK: Generate Synthetic Test Cases with DeepEval Synthesizer\n# =============================================================================\n# Generate test cases automatically from documentation files.\n\nfrom deepeval.test_case import LLMTestCase\n\nprint(\"Generating synthetic test cases from documents...\")\nprint(\"=\" * 60)\n\n# Generate goldens from document files\ngoldens = synthesizer.generate_goldens_from_docs(\n    document_paths=document_paths,\n    include_expected_output=True,\n    max_goldens_per_context=2,\n)\n\nprint(f\"\\nGenerated {len(goldens)} golden test cases\")\nprint(\"-\" * 60)\n\n# Convert goldens to LLMTestCase with actual DevHub responses\nsynthetic_test_cases = []\n\nfor i, golden in enumerate(goldens):\n    print(f\"\\n[{i+1}/{len(goldens)}] Processing: {golden.input[:50]}...\")\n    \n    # Get actual response from DevHub\n    actual_response = devhub.query(golden.input)\n    \n    test_case = LLMTestCase(\n        input=golden.input,\n        actual_output=actual_response,\n        expected_output=golden.expected_output\n    )\n    synthetic_test_cases.append(test_case)\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"Total synthetic test cases: {len(synthetic_test_cases)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Evaluate Synthetic Test Cases\n# =============================================================================\n# Run our metrics on the synthetic test cases.\n\nfrom deepeval import evaluate\nimport time\n\nprint(\"Evaluating synthetic test cases...\")\nprint(\"=\" * 60)\nstart_time = time.time()\n\n# Use the metrics from Lab 1\nresults = evaluate(synthetic_test_cases, all_metrics)\n\n# Summarize\npassed = 0\nfailed = 0\n\nfor test_case in synthetic_test_cases:\n    test_passed = True\n    for metric in all_metrics:\n        metric.measure(test_case)\n        if metric.score < metric.threshold:\n            test_passed = False\n    if test_passed:\n        passed += 1\n    else:\n        failed += 1\n\nelapsed = time.time() - start_time\n\nprint(f\"\\nSynthetic Test Results:\")\nprint(f\"   Total: {len(synthetic_test_cases)}\")\nprint(f\"   Passed: {passed}\")\nprint(f\"   Failed: {failed}\")\nprint(f\"   Pass Rate: {passed/len(synthetic_test_cases)*100:.1f}%\")\nprint(f\"   Time: {elapsed:.1f} seconds\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Regression Testing for Prompts\n\n### What is Regression Testing?\n\nEnsuring that **changes don't break existing functionality**.\n\nFor LLMs, this means:\n- New prompt version should pass all existing tests\n- Quality scores should not decrease significantly\n- Edge cases should still be handled correctly\n\n### The Regression Testing Pipeline\n\n1. **Establish Baseline**: Run tests on current prompt version\n2. **Make Changes**: Modify the prompt\n3. **Run Tests Again**: Same tests, new prompt\n4. **Compare Results**: Did scores improve or regress?\n5. **Decision**: Accept or reject the change",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# REGRESSION TESTING: Capture Baseline\n# =============================================================================\n# Establish baseline scores before making changes.\n\ndef capture_baseline(test_cases: list, metrics: list) -> dict:\n    \"\"\"Capture baseline scores for regression testing.\"\"\"\n    baseline = {\n        \"total_tests\": len(test_cases),\n        \"scores_by_metric\": {},\n        \"test_results\": []\n    }\n\n    for test_case in test_cases:\n        test_result = {\"input\": test_case.input, \"scores\": {}}\n\n        for metric in metrics:\n            metric.measure(test_case)\n\n            if metric.name not in baseline[\"scores_by_metric\"]:\n                baseline[\"scores_by_metric\"][metric.name] = []\n\n            baseline[\"scores_by_metric\"][metric.name].append(metric.score)\n            test_result[\"scores\"][metric.name] = metric.score\n\n        baseline[\"test_results\"].append(test_result)\n\n    # Calculate averages\n    baseline[\"averages\"] = {}\n    for metric_name, scores in baseline[\"scores_by_metric\"].items():\n        baseline[\"averages\"][metric_name] = sum(scores) / len(scores)\n\n    return baseline\n\n# Capture baseline\nprint(\"Capturing baseline scores...\")\nbaseline = capture_baseline(all_test_cases[:5], all_metrics)  # Use subset for demo\n\nprint(\"\\nBaseline Established:\")\nfor metric_name, avg in baseline[\"averages\"].items():\n    print(f\"   {metric_name}: {avg:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Regression Testing Flow\n\n![Regression Testing Flow](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_02/charts/06_regression_testing_flow.svg)\n\n**Key Decision Points:**\n\n1. **Score Comparison**\n   - If new scores >= baseline: Consider accepting\n   - If new scores < baseline: Investigate regression\n\n2. **Threshold for Regression**\n   - Typical: 5-10% decrease is a warning\n   - Critical metrics: Any decrease is a failure\n\n3. **Handling Trade-offs**\n   - One metric improves, another regresses?\n   - Document and decide based on priorities",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# REGRESSION TESTING: Compare Results\n# =============================================================================\n# Compare new results against baseline.\n\ndef compare_with_baseline(new_results: dict, baseline: dict, regression_threshold: float = 0.05) -> dict:\n    \"\"\"Compare new results with baseline and detect regressions.\"\"\"\n    comparison = {\n        \"regressions\": [],\n        \"improvements\": [],\n        \"unchanged\": [],\n        \"summary\": {}\n    }\n\n    for metric_name in baseline[\"averages\"]:\n        baseline_avg = baseline[\"averages\"][metric_name]\n        new_avg = new_results[\"averages\"][metric_name]\n        diff = new_avg - baseline_avg\n        pct_change = (diff / baseline_avg) * 100 if baseline_avg > 0 else 0\n\n        result = {\n            \"metric\": metric_name,\n            \"baseline\": baseline_avg,\n            \"new\": new_avg,\n            \"diff\": diff,\n            \"pct_change\": pct_change\n        }\n\n        if diff < -regression_threshold:\n            comparison[\"regressions\"].append(result)\n        elif diff > regression_threshold:\n            comparison[\"improvements\"].append(result)\n        else:\n            comparison[\"unchanged\"].append(result)\n\n    comparison[\"summary\"] = {\n        \"total_metrics\": len(baseline[\"averages\"]),\n        \"regressions\": len(comparison[\"regressions\"]),\n        \"improvements\": len(comparison[\"improvements\"]),\n        \"unchanged\": len(comparison[\"unchanged\"]),\n        \"overall\": \"PASS\" if len(comparison[\"regressions\"]) == 0 else \"FAIL\"\n    }\n\n    return comparison\n\n# Simulate a \"new version\" (in practice, you'd change the prompt)\nprint(\"Simulating prompt change and re-running tests...\")\nnew_results = capture_baseline(all_test_cases[:5], all_metrics)\n\n# Compare\ncomparison = compare_with_baseline(new_results, baseline)\n\nprint(\"\\nRegression Test Results:\")\nprint(\"=\" * 60)\nprint(f\"Overall: {comparison['summary']['overall']}\")\nprint(f\"   Regressions: {comparison['summary']['regressions']}\")\nprint(f\"   Improvements: {comparison['summary']['improvements']}\")\nprint(f\"   Unchanged: {comparison['summary']['unchanged']}\")\n\nif comparison[\"regressions\"]:\n    print(\"\\nRegressions Detected:\")\n    for r in comparison[\"regressions\"]:\n        print(f\"   {r['metric']}: {r['baseline']:.2f} → {r['new']:.2f} ({r['pct_change']:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# COMPLETE REGRESSION TESTING PIPELINE\n# =============================================================================\n# Put it all together into a reusable function.\n\ndef run_regression_test(\n    test_cases: list,\n    metrics: list,\n    baseline: dict = None,\n    regression_threshold: float = 0.05\n) -> dict:\n    \"\"\"Run a complete regression test.\n\n    Args:\n        test_cases: List of LLMTestCase to evaluate\n        metrics: List of metrics to use\n        baseline: Previous baseline (if None, just capture)\n        regression_threshold: Threshold for detecting regression\n\n    Returns:\n        Dict with results and comparison\n    \"\"\"\n    # Capture current results\n    current = capture_baseline(test_cases, metrics)\n\n    if baseline is None:\n        return {\n            \"mode\": \"baseline_capture\",\n            \"results\": current,\n            \"message\": \"Baseline captured. Save this for future comparisons.\"\n        }\n\n    # Compare with baseline\n    comparison = compare_with_baseline(current, baseline, regression_threshold)\n\n    return {\n        \"mode\": \"comparison\",\n        \"results\": current,\n        \"comparison\": comparison,\n        \"passed\": comparison[\"summary\"][\"overall\"] == \"PASS\"\n    }\n\n# Demo the pipeline\nprint(\"Running regression test pipeline...\")\nresult = run_regression_test(all_test_cases[:5], all_metrics, baseline=baseline)\n\nprint(f\"\\nPipeline Result: {'PASSED' if result['passed'] else 'FAILED'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Lab 2 Verification Checklist\n\n### 1. Synthetic Data Generation\n- [ ] Generated paraphrases for seed queries\n- [ ] Created 9+ synthetic test cases\n- [ ] Verified synthetic cases have expected_output\n\n### 2. Synthetic Evaluation\n- [ ] Ran metrics on synthetic test cases\n- [ ] Calculated pass rate\n- [ ] Identified any failing cases\n\n### 3. Regression Testing\n- [ ] Captured baseline scores\n- [ ] Ran comparison against baseline\n- [ ] Understood regression vs improvement detection\n\n### 4. Pipeline\n- [ ] Regression pipeline returns pass/fail\n- [ ] Can identify which metrics regressed\n- [ ] Ready to integrate into CI/CD\n\n**If all checked, you've built a regression testing system!**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Taking It Further: CI/CD Integration\n\n```yaml\n# .github/workflows/llm-regression.yml\nname: LLM Regression Tests\non: [pull_request]\n\njobs:\n  regression-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: pip install deepeval openai\n\n      - name: Load baseline\n        run: |\n          # Load saved baseline from artifact or file\n          python scripts/load_baseline.py\n\n      - name: Run regression tests\n        run: |\n          python -c \"\n          from regression import run_regression_test\n          result = run_regression_test(test_cases, metrics, baseline)\n          if not result['passed']:\n              raise Exception('Regression detected!')\n          \"\n\n      - name: Update baseline on merge\n        if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n        run: python scripts/save_baseline.py\n```\n\n**This ensures:**\n- Every PR is tested for regressions\n- Baseline is updated after merges\n- No surprises in production",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Lab 2 Key Takeaways\n\n1. **Synthetic data expands test coverage** automatically (10 → 100 cases)\n\n2. **Paraphrasing catches wording variations** you wouldn't think of\n\n3. **Regression testing compares prompt versions** systematically\n\n4. **Baselines establish quality benchmarks** for comparison\n\n5. **CI/CD integration prevents regressions** from reaching production\n\n---\n\n**Next:** Wrap-up and take-home exercise.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Session 2: Wrap-Up\n\n## What You Learned Today\n\n### 1. Why Traditional Testing Fails\n- Non-deterministic outputs\n- Multi-dimensional quality\n- Scale challenges\n\n### 2. DeepEval Framework\n- Test cases as units of evaluation\n- Pytest-like interface\n- 50+ research-backed metrics\n\n### 3. G-Eval Mastery\n- Custom criteria in plain English\n- Chain-of-thought evaluation\n- 0.87 correlation with humans\n\n### 4. Practical Skills\n- Built evaluation harness for DevHub\n- Generated synthetic test cases\n- Ran automated regression tests\n- Caught quality issues systematically",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Before vs After Testing\n\n### Before\n```\nPM: \"I tested 20 prompts manually, looks good!\"\n*Deploys to production*\n2 hours later...\nSupport: \"Everything is broken!\"\n*Emergency rollback at 2am*\n```\n\n### After\n```\nEngineer: \"Running automated evaluation...\"\n*100 tests in 3 minutes*\n\"New prompt passes 98/100 tests but fails 2 refund edge cases.\"\n*Fixes before deployment*\n*Zero production incidents*\n```\n\n**From 2 days of manual testing to 3 minutes of automated evaluation.**",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "![Before/After Testing](https://raw.githubusercontent.com/axel-sirota/salesforce-ai-workshops/main/exercises/session_02/charts/07_before_after_testing.svg)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5 Key Takeaways\n\n### 1. Test Quality, Not Equality\nLLM outputs are non-deterministic. Use quality scoring, not exact matching.\n\n### 2. G-Eval is Your Swiss Army Knife\nDefine any criteria in plain English. It handles the complexity.\n\n### 3. Automation is Essential\nManual testing doesn't scale. 100 automated tests in 3 minutes vs 2 days manual.\n\n### 4. Synthetic Data Expands Coverage\n10 seed examples → 100 test cases automatically.\n\n### 5. Catch Regressions Before Deployment\nRun tests on every prompt change. No surprises in production.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Take-Home Exercise: CI/CD Integration\n\n**Integrate testing into your deployment pipeline.**\n\n### Option 1: GitHub Actions\n```yaml\n# .github/workflows/llm-tests.yml\nname: LLM Quality Tests\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - run: pip install deepeval openai\n      - run: deepeval test run tests/\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n```\n\n### What to Deliver\n1. Set up CI workflow in your project\n2. Run DeepEval tests on every PR\n3. Fail PRs that don't pass threshold\n4. Test by making a breaking change\n\nBring your results to Session 3!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Coming Up: Session 3 - Debugging AI Applications\n\nIn Session 3, we'll learn how to **debug production AI systems** when things go wrong.\n\n### Topics:\n- Distributed tracing for AI workflows\n- Real-time dashboards\n- Root cause analysis\n- Common failure patterns\n\n### You'll Build:\n- Complete observability stack\n- Debugging workflows\n- Alerting system\n\n---\n\n## Congratulations!\n\nYou've completed Session 2: Testing Strategies for AI Applications!\n\n**Skills gained:**\n- Understand why LLM testing is different\n- Build automated evaluation harnesses\n- Use G-Eval for custom quality metrics\n- Generate synthetic test data\n- Run 100 tests in 3 minutes\n- Catch regressions before deployment\n\nSee you in Session 3!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}